{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://paperswithcode.com/paper/facial-emotion-recognition-state-of-the-art"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "from PIL import Image\n",
    "from torch.utils.data import Dataset\n",
    "\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    def __init__(self, images, labels, transform=None, augment=True):\n",
    "        self.images = images\n",
    "        self.labels = labels\n",
    "        self.transform = transform\n",
    "\n",
    "        self.augment = augment\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.images)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        if torch.is_tensor(idx):\n",
    "            idx = idx.tolist()\n",
    "\n",
    "        img = np.array(self.images[idx])\n",
    "\n",
    "        img = Image.fromarray(img)\n",
    "\n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        label = torch.tensor(self.labels[idx]).type(torch.long)\n",
    "        sample = (img, label)\n",
    "\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "\n",
    "def load_data(path='datasets/fer2013/fer2013.csv'):\n",
    "    fer2013 = pd.read_csv(path)\n",
    "    fer2013.columns = ['emotion', 'Usage', 'pixels']\n",
    "    emotion_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Sad', 5: 'Surprise', 6: 'Neutral'}\n",
    "\n",
    "    return fer2013, emotion_mapping\n",
    "\n",
    "\n",
    "def prepare_data(data):\n",
    "    \"\"\" Prepare data for modeling\n",
    "        input: data frame with labels und pixel data\n",
    "        output: image and label array \"\"\"\n",
    "\n",
    "    image_array = np.zeros(shape=(len(data), 48, 48))\n",
    "    image_label = np.array(list(map(int, data['emotion'])))\n",
    "\n",
    "    for i, row in enumerate(data.index):\n",
    "        image = np.fromstring(data.loc[row, 'pixels'], dtype=int, sep=' ')\n",
    "        image = np.reshape(image, (48, 48))\n",
    "        image_array[i] = image\n",
    "\n",
    "    return image_array, image_label\n",
    "\n",
    "\n",
    "def get_dataloaders(path=r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\icml_face_data.csv', bs=64, augment=True):\n",
    "    \"\"\" Prepare train, val, & test dataloaders\n",
    "        Augment training data using:\n",
    "            - cropping\n",
    "            - shifting (vertical/horizental)\n",
    "            - horizental flipping\n",
    "            - rotation\n",
    "\n",
    "        input: path to fer2013 csv file\n",
    "        output: (Dataloader, Dataloader, Dataloader) \"\"\"\n",
    "\n",
    "    fer2013, emotion_mapping = load_data(path)\n",
    "\n",
    "    xtrain, ytrain = prepare_data(fer2013[fer2013['Usage'] == 'Training'])\n",
    "    xval, yval = prepare_data(fer2013[fer2013['Usage'] == 'PrivateTest'])\n",
    "    xtest, ytest = prepare_data(fer2013[fer2013['Usage'] == 'PublicTest'])\n",
    "\n",
    "    mu, st = 0, 255\n",
    "\n",
    "    '''\n",
    "    test_transform = transforms.Compose([\n",
    "        # transforms.Scale(52),\n",
    "        transforms.TenCrop(40),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "    ])\n",
    "    '''\n",
    "    test_transform = transforms.Compose([\n",
    "    transforms.TenCrop(40),\n",
    "    transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "    transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "    transforms.Lambda(lambda tensors: torch.stack([transforms.Resize((48, 48))(t) for t in tensors])),\n",
    "    ])\n",
    "\n",
    "\n",
    "    if augment:\n",
    "        '''\n",
    "        train_transform = transforms.Compose([\n",
    "            transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "            transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "            transforms.RandomHorizontalFlip(),\n",
    "            transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "\n",
    "            transforms.TenCrop(40),\n",
    "            transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "            transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "            transforms.Lambda(lambda tensors: torch.stack([transforms.RandomErasing(p=0.5)(t) for t in tensors])),\n",
    "        ])\n",
    "        '''\n",
    "        train_transform = transforms.Compose([\n",
    "        transforms.RandomResizedCrop(48, scale=(0.8, 1.2)),\n",
    "        transforms.RandomApply([transforms.RandomAffine(0, translate=(0.2, 0.2))], p=0.5),\n",
    "        transforms.RandomHorizontalFlip(),\n",
    "        transforms.RandomApply([transforms.RandomRotation(10)], p=0.5),\n",
    "        transforms.TenCrop(40),\n",
    "        transforms.Lambda(lambda crops: torch.stack([transforms.ToTensor()(crop) for crop in crops])),\n",
    "        transforms.Lambda(lambda tensors: torch.stack([transforms.Normalize(mean=(mu,), std=(st,))(t) for t in tensors])),\n",
    "        transforms.Lambda(lambda tensors: torch.stack([transforms.RandomErasing(p=0.5)(t) for t in tensors])),\n",
    "        transforms.Lambda(lambda tensors: torch.stack([transforms.Resize((48, 48))(t) for t in tensors])),\n",
    "    ])\n",
    "\n",
    "\n",
    "    else:\n",
    "        train_transform = test_transform\n",
    "\n",
    "\n",
    "\n",
    "    # X = np.vstack((xtrain, xval))\n",
    "    # Y = np.hstack((ytrain, yval))\n",
    "\n",
    "    train = CustomDataset(xtrain, ytrain, transform=train_transform)\n",
    "    val = CustomDataset(xval, yval, transform=test_transform)  # Usa test_transform per il set di test\n",
    "    test = CustomDataset(xtest, ytest, transform=test_transform)  # Usa test_transform per il set di test\n",
    "\n",
    "\n",
    "    trainloader = DataLoader(train, batch_size=64, shuffle=True)\n",
    "    valloader = DataLoader(val, batch_size=64, shuffle=True)\n",
    "    testloader = DataLoader(test, batch_size=64, shuffle=True)\n",
    "\n",
    "    return trainloader, valloader, testloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainloader, valloader, testloader = get_dataloaders()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([10, 1, 48, 48])\n",
      "torch.Size([10, 1, 48, 48])\n",
      "torch.Size([10, 1, 48, 48])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torchvision\\transforms\\functional.py:1603: UserWarning: The default value of the antialias parameter of all the resizing transforms (Resize(), RandomResizedCrop(), etc.) will change from None to True in v0.17, in order to be consistent across the PIL and Tensor backends. To suppress this warning, directly pass antialias=True (recommended, future default), antialias=None (current default, which means False for Tensors and True for PIL), or antialias=False (only works on Tensors - PIL will still use antialiasing). This also applies if you are using the inference transforms from the models weights: update the call to weights.transforms(antialias=True).\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "print(trainloader.dataset[0][0].shape)\n",
    "print(valloader.dataset[0][0].shape)\n",
    "print(testloader.dataset[0][0].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(449, 57, 57)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(trainloader), len(valloader), len(testloader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "\n",
    "\n",
    "class VggFeatures(nn.Module):\n",
    "    def __init__(self, drop=0.2):\n",
    "        super().__init__()\n",
    "\n",
    "        self.conv1a = nn.Conv2d(in_channels=1, out_channels=64, kernel_size=3, padding=1)\n",
    "        self.conv1b = nn.Conv2d(64, out_channels=64, kernel_size=3, padding=1)\n",
    "\n",
    "        self.conv2a = nn.Conv2d(64, 128, 3, padding=1)\n",
    "        self.conv2b = nn.Conv2d(128, 128, 3, padding=1)\n",
    "\n",
    "        self.conv3a = nn.Conv2d(128, 256, 3, padding=1)\n",
    "        self.conv3b = nn.Conv2d(256, 256, 3, padding=1)\n",
    "\n",
    "        self.conv4a = nn.Conv2d(256, 512, 3, padding=1)\n",
    "        self.conv4b = nn.Conv2d(512, 512, 3, padding=1)\n",
    "\n",
    "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "        self.bn1a = nn.BatchNorm2d(64)\n",
    "        self.bn1b = nn.BatchNorm2d(64)\n",
    "\n",
    "        self.bn2a = nn.BatchNorm2d(128)\n",
    "        self.bn2b = nn.BatchNorm2d(128)\n",
    "\n",
    "        self.bn3a = nn.BatchNorm2d(256)\n",
    "        self.bn3b = nn.BatchNorm2d(256)\n",
    "\n",
    "        self.bn4a = nn.BatchNorm2d(512)\n",
    "        self.bn4b = nn.BatchNorm2d(512)\n",
    "\n",
    "        self.lin1 = nn.Linear(512 * 2 * 2, 4096)\n",
    "        self.lin2 = nn.Linear(4096, 4096)\n",
    "\n",
    "        self.drop = nn.Dropout(p=drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.bn1a(self.conv1a(x)))\n",
    "        x = F.relu(self.bn1b(self.conv1b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn2a(self.conv2a(x)))\n",
    "        x = F.relu(self.bn2b(self.conv2b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn3a(self.conv3a(x)))\n",
    "        x = F.relu(self.bn3b(self.conv3b(x)))\n",
    "        x = self.pool(x)\n",
    "\n",
    "        x = F.relu(self.bn4a(self.conv4a(x)))\n",
    "        x = F.relu(self.bn4b(self.conv4b(x)))\n",
    "        x = self.pool(x)\n",
    "        # print(x.shape)\n",
    "\n",
    "        x = x.view(-1, 512 * 2 * 2)\n",
    "        x = F.relu(self.drop(self.lin1(x)))\n",
    "        x = F.relu(self.drop(self.lin2(x)))\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class Vgg(VggFeatures):\n",
    "    def __init__(self, drop=0.2):\n",
    "        super().__init__(drop)\n",
    "        self.lin3 = nn.Linear(4096, 7)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = super().forward(x)\n",
    "        x = self.lin3(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "##### GPT ######\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class VGGNet(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(VGGNet, self).__init__()\n",
    "        self.features = nn.Sequential(\n",
    "            nn.Conv2d(1, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(64, 64, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(64, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(128, 128, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(128, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(256, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2),\n",
    "\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Conv2d(512, 512, kernel_size=3, padding=1),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "        )\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(512 * 6 * 6, 4096),  # Adatta questa dimensione in base alle dimensioni del tuo output dalla parte features\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, 4096),\n",
    "            nn.ReLU(inplace=True),\n",
    "            nn.Dropout(),\n",
    "            nn.Linear(4096, num_classes),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Se l'input ha dimensione 5D, ridimensiona prima di passare attraverso la parte convoluzionale\n",
    "        if x.dim() == 5:\n",
    "            batch_size, num_crops, channels, height, width = x.size()\n",
    "            x = x.view(-1, channels, height, width)\n",
    "\n",
    "        x = self.features(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.classifier(x)\n",
    "\n",
    "        # Se l'input ha dimensione 5D, ridimensiona l'output finale\n",
    "        if batch_size > 1 and num_crops > 1:\n",
    "            x = x.view(batch_size, num_crops, -1).mean(1)  # Media sulle dimensioni dei crop\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "# Impostazioni\n",
    "learning_rate = 0.01\n",
    "n_epochs = 10\n",
    "\n",
    "# Modello\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "net = Vgg(drop=0.2)\n",
    "net.to(device)\n",
    "\n",
    "# Ottimizzatore e criterio\n",
    "optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Scheduler\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "\n",
    "# Training loop\n",
    "best_accuracy = 0.0\n",
    "for epoch in range(n_epochs):\n",
    "    net.train()\n",
    "    running_loss = 0.0\n",
    "    all_labels = []\n",
    "    all_predictions = []\n",
    "\n",
    "    for inputs, labels in tqdm(trainloader, desc=f'Epoch {epoch + 1}/{n_epochs}', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        outputs = net(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, predictions = torch.max(outputs, 1)\n",
    "        all_labels.extend(labels.cpu().numpy())\n",
    "        all_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    # Calcolo delle metriche di allenamento\n",
    "    accuracy = accuracy_score(all_labels, all_predictions)\n",
    "    precision = precision_score(all_labels, all_predictions, average='macro')\n",
    "    recall = recall_score(all_labels, all_predictions, average='macro')\n",
    "\n",
    "    # Calcolo delle metriche di validazione\n",
    "    net.eval()\n",
    "    val_labels = []\n",
    "    val_predictions = []\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in valloader:\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = net(inputs)\n",
    "            val_loss += criterion(outputs, labels).item()\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            val_labels.extend(labels.cpu().numpy())\n",
    "            val_predictions.extend(predictions.cpu().numpy())\n",
    "\n",
    "    val_accuracy = accuracy_score(val_labels, val_predictions)\n",
    "    val_precision = precision_score(val_labels, val_predictions, average='macro')\n",
    "    val_recall = recall_score(val_labels, val_predictions, average='macro')\n",
    "\n",
    "    # Aggiornamento dello scheduler\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    # Salvataggio del modello se l'accuracy di validazione è migliorata\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        torch.save(net.state_dict(), r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper2_models\\model0.pth')\n",
    "\n",
    "    # Stampa delle metriche\n",
    "    print(f\"Epoch {epoch + 1}/{n_epochs} - Loss: {running_loss / len(trainloader):.4f} - \"\n",
    "          f\"Accuracy: {accuracy:.4f} - Precision: {precision:.4f} - Recall: {recall:.4f} - \"\n",
    "          f\"Val Loss: {val_loss / len(valloader):.4f} - \"\n",
    "          f\"Val Accuracy: {val_accuracy:.4f} - Val Precision: {val_precision:.4f} - Val Recall: {val_recall:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/3 - Training:   0%|          | 0/449 [00:05<?, ?it/s]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "mat1 and mat2 shapes cannot be multiplied (640x512 and 18432x4096)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 84\u001b[0m\n\u001b[0;32m     81\u001b[0m scheduler \u001b[38;5;241m=\u001b[39m ReduceLROnPlateau(optimizer, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmax\u001b[39m\u001b[38;5;124m'\u001b[39m, factor\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0.75\u001b[39m, patience\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m, verbose\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[0;32m     83\u001b[0m \u001b[38;5;66;03m# Chiama la funzione di allenamento\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrainloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvalloader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlearning_rate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mscheduler\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[25], line 35\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, trainloader, valloader, epochs, learning_rate, optimizer, scheduler)\u001b[0m\n\u001b[0;32m     31\u001b[0m inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m     33\u001b[0m optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[1;32m---> 35\u001b[0m outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     36\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     37\u001b[0m loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[24], line 64\u001b[0m, in \u001b[0;36mVGGNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     62\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfeatures(x)\n\u001b[0;32m     63\u001b[0m x \u001b[38;5;241m=\u001b[39m x\u001b[38;5;241m.\u001b[39mview(x\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m---> 64\u001b[0m x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclassifier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m \u001b[38;5;66;03m# Se l'input ha dimensione 5D, ridimensiona l'output finale\u001b[39;00m\n\u001b[0;32m     67\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m batch_size \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m num_crops \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\container.py:215\u001b[0m, in \u001b[0;36mSequential.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    213\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m):\n\u001b[0;32m    214\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m module \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m:\n\u001b[1;32m--> 215\u001b[0m         \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[43mmodule\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    216\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28minput\u001b[39m\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: mat1 and mat2 shapes cannot be multiplied (640x512 and 18432x4096)"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score\n",
    "from tqdm import tqdm\n",
    "\n",
    "model = VGGNet()\n",
    "\n",
    "# Funzioni per calcolare accuracy, precision e recall\n",
    "def calculate_metrics(predictions, labels):\n",
    "    accuracy = accuracy_score(labels, predictions)\n",
    "    precision = precision_score(labels, predictions, average=None)\n",
    "    recall = recall_score(labels, predictions, average=None)\n",
    "    return accuracy, precision, recall\n",
    "\n",
    "# Funzione per l'allenamento del modello\n",
    "def train(model, trainloader, valloader, epochs, learning_rate, optimizer, scheduler):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    model.to(device)\n",
    "    \n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        predictions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        for inputs, labels in tqdm(trainloader, desc=f'Epoch {epoch + 1}/{epochs} - Training'):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, predictions = torch.max(outputs, 1)\n",
    "            predictions_list.extend(predictions.cpu().numpy())\n",
    "            labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calcola e stampa le metriche di allenamento\n",
    "        accuracy_train, precision_train, recall_train = calculate_metrics(predictions_list, labels_list)\n",
    "        print(f'Training Accuracy: {accuracy_train:.4f}')\n",
    "        print(f'Training Precision: {precision_train}')\n",
    "        print(f'Training Recall: {recall_train}')\n",
    "\n",
    "        # Valutazione sul set di validazione\n",
    "        model.eval()\n",
    "        predictions_list = []\n",
    "        labels_list = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for inputs, labels in tqdm(valloader, desc=f'Epoch {epoch + 1}/{epochs} - Validation'):\n",
    "                inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "                outputs = model(inputs)\n",
    "                _, predictions = torch.max(outputs, 1)\n",
    "                predictions_list.extend(predictions.cpu().numpy())\n",
    "                labels_list.extend(labels.cpu().numpy())\n",
    "\n",
    "        # Calcola e stampa le metriche di validazione\n",
    "        accuracy_val, precision_val, recall_val = calculate_metrics(predictions_list, labels_list)\n",
    "        print(f'Validation Accuracy: {accuracy_val:.4f}')\n",
    "        print(f'Validation Precision: {precision_val}')\n",
    "        print(f'Validation Recall: {recall_val}')\n",
    "\n",
    "        # Aggiorna il rate del learning scheduler\n",
    "        scheduler.step(accuracy_val)\n",
    "\n",
    "    print(\"Training complete.\")\n",
    "\n",
    "# Parametri di allenamento\n",
    "learning_rate = 0.01\n",
    "epochs = 3\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "\n",
    "# Chiama la funzione di allenamento\n",
    "train(model, trainloader, valloader, epochs, learning_rate, optimizer, scheduler)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
