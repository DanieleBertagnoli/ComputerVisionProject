{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WINDOWS (your env must be called project-venv; if you choose another name add it in .gitignore)\n",
    "import subprocess\n",
    "\n",
    "# Set the execution policy\n",
    "subprocess.run([\"Set-ExecutionPolicy\", \"RemoteSigned\", \"-Scope\", \"Process\"], shell=True)\n",
    "\n",
    "# Activate the virtual environment\n",
    "subprocess.run([\"cd\", \".\\\\project-venv\\\\\"], shell=True)\n",
    "subprocess.run([\".\\\\Scripts\\\\Activate.ps1\"], shell=True)\n",
    "\n",
    "# Install requirements\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../requirements.txt\"], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Emotion Recognition task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import dlib\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wandb login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='fdaf4406bdf6e269b286a9346e1594b1308956cc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Paper implementation 1**\n",
    "https://ieeexplore.ieee.org/abstract/document/9659697?casa_token=zDD7lwwOig8AAAAA:KcIHhupXAXgiaB_C7A0uNDB7ehrsWNyovQdgDu9LmnwToOGU6akB_gjWTy7JCf4UdKK03Is\n",
    "\n",
    "https://github.com/serengil/deepface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Augmenting**\n",
    "\n",
    "#### **Objective:**\n",
    "The purpose of this Python script is to perform data augmentation on a dataset of images corresponding to different emotions. Data augmentation is a technique commonly used in computer vision tasks to increase the diversity of the training dataset, thereby enhancing the robustness and generalization capabilities of machine learning models.\n",
    "\n",
    "#### **Dataset Structure:**\n",
    "The original dataset is organized into folders, each representing a specific emotion (e.g., angry, disgust, fear, happy, neutral, sad, surprise). Each emotion folder contains a collection of images in formats such as JPEG and PNG.\n",
    "\n",
    "#### **Transformation Techniques:**\n",
    "The script employs various image transformation techniques to augment the dataset. These techniques include:\n",
    "\n",
    "1. **Horizontal Flip:** Flips the image horizontally.\n",
    "2. **Vertical Flip:** Flips the image vertically.\n",
    "3. **Zoom:** Randomly zooms into or out of the image.\n",
    "4. **Translation:** Shifts the image horizontally and vertically.\n",
    "5. **Contrast and Brightness Adjustment:** Randomly adjusts the contrast and brightness of the image.\n",
    "6. **Elastic Transformation:** Applies a non-linear elastic deformation to the image.\n",
    "\n",
    "#### **Implementation:**\n",
    "The script uses the OpenCV library for image processing. For each emotion category, it iterates through the images, applies the defined transformations using the `apply_transformations` function, and saves the augmented images in a new folder named with the emotion followed by \"_augmented.\"\n",
    "\n",
    "The `elastic_transform` function generates an elastic transformation by displacing pixels based on random elastic deformations. This adds a degree of distortion to the images, contributing to further variability.\n",
    "\n",
    "#### **Output:**\n",
    "The augmented images are saved in the corresponding emotion's augmented folder, with filenames indicating the applied augmentation technique. For example, an image originally named \"example_image.jpg\" may result in augmented images like \"example_image_aug_0.jpg,\" \"example_image_aug_1.jpg,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# cycle through emotions\n",
    "for emotion in emotions:\n",
    "    # path of the folder containing the images\n",
    "    folder_path = f\"../../../Data/emotions_images/{emotion}\"\n",
    "    output_folder_path = f\"../../../Data/emotions_aug_images/{emotion}_augmented\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # list of images in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # define transformations inside the apply_transformations function\n",
    "    def apply_transformations(image):\n",
    "        # horizontal_flip\n",
    "        flipped_horizontal = cv2.flip(image, 1)\n",
    "\n",
    "        # vertical flip\n",
    "        flipped_vertical = cv2.flip(image, 0)\n",
    "\n",
    "        # Zoom\n",
    "        zoom_factor = random.uniform(0.8, 1.2)\n",
    "        height, width = image.shape[:2]\n",
    "        zoomed_image = cv2.resize(image, (int(width * zoom_factor), int(height * zoom_factor)))\n",
    "\n",
    "        # translation\n",
    "        tx = random.randint(-10, 10)\n",
    "        ty = random.randint(-10, 10)\n",
    "        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        translated_image = cv2.warpAffine(image, translation_matrix, (width, height))\n",
    "\n",
    "        # contrast and brightness control\n",
    "        alpha = random.uniform(0.8, 1.2)\n",
    "        beta = random.randint(-35, 35)\n",
    "        adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "        # elastic transformation\n",
    "        elastic_image = elastic_transform(image, alpha=random.randint(6, 14), sigma=random.uniform(1.1, 2.0))\n",
    "\n",
    "        return [image, translated_image, flipped_horizontal, zoomed_image, adjusted_image, elastic_image] #forse togliere flipped vertical\n",
    "\n",
    "    def elastic_transform(image, alpha, sigma):\n",
    "        random_state = np.random.RandomState(None)\n",
    "        shape = image.shape\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dz = np.zeros_like(dx)\n",
    "\n",
    "        x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "        indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "\n",
    "        distorted_image = map_coordinates(image, indices, order=1, mode='reflect')\n",
    "        distorted_image = distorted_image.reshape(image.shape)\n",
    "\n",
    "        return distorted_image\n",
    "\n",
    "    # apply data augmentation\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        augmented_images = apply_transformations(image)\n",
    "\n",
    "        # save new images\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        for i, augmented_image in enumerate(augmented_images):\n",
    "            output_file_path = os.path.join(output_folder_path, f\"{base_name}_aug_{i}.jpg\")\n",
    "            cv2.imwrite(output_file_path, augmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Architecture: EmotionCNN**\n",
    "\n",
    "#### **Overview:**\n",
    "The `EmotionCNN` model is designed for emotion classification using a Convolutional Neural Network (CNN). Below is an overview of its architecture:\n",
    "\n",
    "#### **Architecture Layers:**\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - Input channels: 1 (grayscale images)\n",
    "   - Input size: Variable (depends on the input image size)\n",
    "\n",
    "2. **Convolutional Layer 1:**\n",
    "   - Convolutional operation: 64 filters of size 5x5\n",
    "   - Activation function: ReLU\n",
    "   - Max pooling: 5x5 kernel with stride 2\n",
    "\n",
    "3. **Convolutional Layers 2a and 2b:**\n",
    "   - Convolutional operation: 64 filters of size 3x3\n",
    "   - Activation functions: ReLU for both layers\n",
    "   - Average pooling: 3x3 kernel with stride 2\n",
    "\n",
    "4. **Convolutional Layers 3a and 3b:**\n",
    "   - Convolutional operation: 128 filters of size 3x3\n",
    "   - Activation functions: ReLU for both layers\n",
    "   - Average pooling: 3x3 kernel with stride 2\n",
    "\n",
    "5. **Fully Connected (FC) Layer 1:**\n",
    "   - Units: 1024\n",
    "   - Activation function: ReLU\n",
    "   - Dropout: 20% dropout rate\n",
    "\n",
    "6. **Fully Connected (FC) Layer 2:**\n",
    "   - Units: 1024\n",
    "   - Activation function: ReLU\n",
    "   - Dropout: 20% dropout rate\n",
    "\n",
    "7. **Output Layer:**\n",
    "   - Units: Variable (depends on the number of emotion classes)\n",
    "   - Activation function: Softmax\n",
    "\n",
    "#### **Additional Information:**\n",
    "- The architecture is designed for grayscale images (1 channel).\n",
    "- The network utilizes ReLU activation functions after convolutional and fully connected layers.\n",
    "- Max pooling and average pooling layers are employed for down-sampling.\n",
    "- Dropout layers (20% dropout rate) are included for regularization.\n",
    "- The final layer uses the softmax activation for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2a = nn.ReLU()\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2b = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3a = nn.ReLU()\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3b = nn.ReLU()\n",
    "        self.avgpool3 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # verify the output size of conv2 and conv3\n",
    "        self.dummy_input = torch.randn(1, 1, 48, 48)\n",
    "        self.dummy_output_size = self._get_conv_output_size(self.dummy_input)\n",
    "        \n",
    "        # update fc1 units based on feature map size\n",
    "        self.fc1 = nn.Linear(self.dummy_output_size, 1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output_size(self, input_tensor):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(input_tensor)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(self.relu_fc1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu_fc2(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameters**\n",
    "\n",
    "The parameters that can be modified before performing training are the number of instances per label, the batch size and the number of epochs (in this case they are managed by wandb)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#number_instances_over_under_sampling_ = 30000\n",
    "#batch_size_ = 48\n",
    "#epochs_ = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Delete Outliers with DBSCAN**\n",
    "\n",
    "#### **Objective:**\n",
    "The Python script is designed to identify and remove outliers from a collection of images using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. The primary goal is to filter out images with unusual pixel statistics, specifically focusing on standard deviation.\n",
    "\n",
    "#### **Functions:**\n",
    "\n",
    "1. **`calculate_pixel_std(image_path):`**\n",
    "   - **Input:** Path to an image file.\n",
    "   - **Output:** Returns the standard deviation of pixel values in the grayscale image.\n",
    "\n",
    "2. **`remove_outliers_dbscan(folder_path, eps, min_samples):`**\n",
    "   - **Input:**\n",
    "     - `folder_path`: Path to the folder containing images.\n",
    "     - `eps`: DBSCAN search radius parameter.\n",
    "     - `min_samples`: Minimum number of samples required for a cluster in DBSCAN.\n",
    "   - **Action:**\n",
    "     - Calculates the pixel standard deviation for each image in the folder.\n",
    "     - Applies DBSCAN to cluster images based on pixel standard deviation.\n",
    "     - Removes outliers (images with label -1) by deleting them from the folder.\n",
    "   - **Output:** Prints the number of outliers removed.\n",
    "\n",
    "#### **Usage:**\n",
    "\n",
    "1. Set the DBSCAN configuration parameters:\n",
    "   - `dbscan_eps`: DBSCAN search radius.\n",
    "   - `dbscan_min_samples`: Minimum number of samples required for a cluster.\n",
    "\n",
    "2. Provide the path to the folder containing emotion-specific image folders (`emotions_folder_path`).\n",
    "\n",
    "3. Iterate through each emotion folder:\n",
    "   - If the emotion is 'disgust,' use different DBSCAN parameters.\n",
    "   - Apply `remove_outliers_dbscan` to delete outliers in each emotion folder.\n",
    "\n",
    "4. The script prints the number of outliers removed for each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def remove_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # remove outliers\n",
    "    counter = 0\n",
    "    for i, (label, image) in enumerate(zip(labels, os.listdir(folder_path))):\n",
    "        if label == -1:  \n",
    "            image_path = os.path.join(folder_path, image)\n",
    "            os.remove(image_path)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "# DBSCAN configuration\n",
    "dbscan_eps = 0.5  # search radius\n",
    "dbscan_min_samples = 10  # minimum number of samples required for a cluster\n",
    "\n",
    "emotions_folder_path = \"../../../Data/emotions_aug_images\"\n",
    "\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "    \n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        remove_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        remove_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the path and the number of the outliers per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def get_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # collect outlier paths and count for each label\n",
    "    outlier_paths_by_label = {}\n",
    "    for label, image_path in zip(labels, image_paths):\n",
    "        if label == -1:\n",
    "            if label not in outlier_paths_by_label:\n",
    "                outlier_paths_by_label[label] = []\n",
    "            outlier_paths_by_label[label].append(image_path)\n",
    "\n",
    "    return outlier_paths_by_label\n",
    "\n",
    "# Configurazione DBSCAN\n",
    "dbscan_eps = 0.5  # Raggio di ricerca\n",
    "dbscan_min_samples = 10  # Numero minimo di campioni in un cluster\n",
    "\n",
    "emotions_folder_path = \"../../../Data/emotions_aug_images\"\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "\n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        outliers = get_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        outliers = get_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n",
    "\n",
    "    # Stampa i percorsi degli outliers per ogni label\n",
    "    for label, outlier_paths in outliers.items():\n",
    "        print(f\"Label {label}: {len(outlier_paths)} outliers\")\n",
    "        for path in outlier_paths:\n",
    "            print(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Emotion Recognition Training and Evaluation**\n",
    "\n",
    "This code performs emotion recognition using a Convolutional Neural Network (CNN). Key components include:\n",
    "\n",
    "- **Emotion Label Mapping**: Emotions are mapped to numerical values for training and evaluation.\n",
    "- **Model Definition and Setup**: A CNN model, along with the criterion, optimizer, and scheduler, is defined.\n",
    "- **Metrics Calculation**: The `calculate_metrics_per_class` function computes accuracy, precision, recall, F1 score, and support for each class.\n",
    "- **Training Function**: The `train_epoch` function trains the model for one epoch, logging loss, accuracy, and metrics per class.\n",
    "- **Evaluation Function**: The `evaluate` function assesses the model on the validation set, calculating loss, accuracy, and metrics per class.\n",
    "- **Test Function**: The `test` function evaluates the model on the test set, providing loss, accuracy, and metrics per class.\n",
    "- **Training and Evaluation Process**: The `train_and_evaluate` function orchestrates the entire process. It initializes WandB, sets up data transformations, creates weighted random samplers for handling class imbalances, and conducts training and validation. The best model is saved, and evaluation metrics are logged.\n",
    "- **Hyperparameter Sweep Configuration**: The code supports hyperparameter optimization using WandB sweeps. Parameters include the number of instances for over/under-sampling, batch size, and epochs.\n",
    "\n",
    "#### Data Preprocessing\n",
    "\n",
    "Two datasets are utilized with different transformations, addressing grayscale, resizing, and normalization. Weighted random samplers handle class imbalances.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "The training process involves iterating through epochs, training on the specified dataset, and evaluating on validation data. Early stopping is implemented to prevent overfitting.\n",
    "\n",
    "#### Logging and Visualization\n",
    "\n",
    "WandB is integrated for experiment tracking. Metrics such as loss, accuracy, and metrics per class are logged during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "\n",
    "def calculate_metrics_per_class(true_labels, predicted_labels, label_mapping):\n",
    "    unique_labels = list(label_mapping.keys())\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, labels=unique_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    metrics_per_class = {}\n",
    "    for i, idx in enumerate(unique_labels):\n",
    "        metrics_per_class[idx] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "\n",
    "    return accuracy, metrics_per_class\n",
    "\n",
    "# function for training\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, label_mapping):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# function for evaluation\n",
    "def evaluate(model, val_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(val_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# Funzione per il test\n",
    "def test(model, test_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(test_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "\n",
    "def train_and_evaluate():\n",
    "    wandb.init(project=\"provaA1\")\n",
    "\n",
    "    number_instances_over_under_sampling_ = wandb.config.get('number_instances_over_under_sampling_')\n",
    "    batch_size_ = wandb.config.get('batch_size_')\n",
    "    epochs_ = wandb.config.get('epochs_')\n",
    "\n",
    "    # definition of the model, criterion, optimizer and scheduler\n",
    "    net = EmotionCNN().to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "\n",
    "        # transformation definition\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\test_images_emotion'\n",
    "\n",
    "    # create an instance of ImageFolder with the transformations\n",
    "    dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "    # seed = 42\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # extract the labels and the indices of the dataset\n",
    "    labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "    # convert the list into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # calculate the number of instances for each class\n",
    "    counts = torch.bincount(labels)\n",
    "\n",
    "    # calculate the weights for each class\n",
    "    weights = 1.0 / counts.float()\n",
    "\n",
    "    # create a weight vector for each index in the dataset\n",
    "    sample_weights = weights[labels]\n",
    "\n",
    "    # set the number of samples for the train set and the test set\n",
    "    train_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "    val_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "    test_size = (number_instances_over_under_sampling_/10) * 7 * 0.8\n",
    "\n",
    "    # crea un sampler per il train set and one for the test set\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "    val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "    test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "    # create a dataloader for the train set and the test set with the corresponding samplers\n",
    "    train_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "    val_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "    test_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "            # transformation definition\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images'\n",
    "\n",
    "    # create an instance of ImageFolder with the transformations\n",
    "    dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "    # seed = 42\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # extract the labels and the indices of the dataset\n",
    "    labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "    # convert the list into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # calculate the number of instances for each class\n",
    "    counts = torch.bincount(labels)\n",
    "\n",
    "    # calculate the weights for each class\n",
    "    weights = 1.0 / counts.float()\n",
    "\n",
    "    # create a weight vector for each index in the dataset\n",
    "    sample_weights = weights[labels]\n",
    "\n",
    "    # set the number of samples for the train set and the test set\n",
    "    train_size = number_instances_over_under_sampling_ * 7 * 0.8\n",
    "    val_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "    test_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "\n",
    "    # crea un sampler per il train set and one for the test set\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "    val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "    test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "    # create a dataloader for the train set and the test set with the corresponding samplers\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "    your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "    # Settings\n",
    "    num_epochs = epochs_\n",
    "    early_stopping_patience = 3  # numbers of epochs with no improvement after which training will be stopped (early stopping)\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    train_metrics_per_class = {}\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    val_metrics_per_class = {}\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_accuracy, train_metrics_per_class = train_epoch(net, train_loader, criterion, optimizer, device, your_label_mapping)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy, val_metrics_per_class = evaluate(net, val_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "        # Scheduler step based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        # Saving the model if the current accuracy is better than the best\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_epoch = epoch\n",
    "            torch.save(net.state_dict(), r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model.pth')\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} => '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        # Print metrics per class\n",
    "        for idx, label in your_label_mapping.items():\n",
    "            print(f'{label}: Train Precision: {train_metrics_per_class[idx][\"precision\"]:.4f}, Train Recall: {train_metrics_per_class[idx][\"recall\"]:.4f}, Train F1: {train_metrics_per_class[idx][\"f1\"]:.4f}, Train Support: {train_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        for idx, label in your_label_mapping.items():\n",
    "            print(f'{label}: Validation Precision: {val_metrics_per_class[idx][\"precision\"]:.4f}, Validation Recall: {val_metrics_per_class[idx][\"recall\"]:.4f}, Validation F1: {val_metrics_per_class[idx][\"f1\"]:.4f}, Validation Support: {val_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1} as there is no improvement in validation accuracy for {early_stopping_patience} consecutive epochs.')\n",
    "            break\n",
    "\n",
    "    print(f'Best model achieved at epoch {best_epoch + 1} with accuracy {best_accuracy:.4f}')\n",
    "\n",
    "    best_model = EmotionCNN()\n",
    "    best_model.load_state_dict(torch.load(r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "    test_loss, test_accuracy, test_metrics_per_class = test(best_model, test_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "    best_model = EmotionCNN()\n",
    "    best_model.load_state_dict(torch.load(r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model.pth'))\n",
    "    best_model.to(device)\n",
    "    criterion = nn.CrossEntropyLoss()\n",
    "    optimizer = SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "    scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "    test_loss_, test_accuracy_, test_metrics_per_class_ = test(best_model, test_loader_, criterion, device, your_label_mapping)\n",
    "\n",
    "    print(\"TEST ACCURACY_: \", test_accuracy_)\n",
    "    print(\"TEST ACCURACY: \", test_accuracy)\n",
    "\n",
    "    # log the best model\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"test_accuracy\": test_accuracy, \"test_accuracy_\": test_accuracy_})\n",
    "\n",
    "    # log metrics per class\n",
    "    for idx, label in your_label_mapping.items():\n",
    "        wandb.log({\n",
    "            f'train_precision_{label}': train_metrics_per_class[idx][\"precision\"],\n",
    "            f'train_recall_{label}': train_metrics_per_class[idx][\"recall\"],\n",
    "            f'train_f1_{label}': train_metrics_per_class[idx][\"f1\"],\n",
    "            f'train_support_{label}': train_metrics_per_class[idx][\"support\"],\n",
    "        })\n",
    "\n",
    "    for idx, label in your_label_mapping.items():\n",
    "        wandb.log({\n",
    "            f'val_precision_{label}': val_metrics_per_class[idx][\"precision\"],\n",
    "            f'val_recall_{label}': val_metrics_per_class[idx][\"recall\"],\n",
    "            f'val_f1_{label}': val_metrics_per_class[idx][\"f1\"],\n",
    "            f'val_support_{label}': val_metrics_per_class[idx][\"support\"],\n",
    "        })\n",
    "\n",
    "\n",
    "# sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        \"number_instances_over_under_sampling_\": {\"values\": [20000]},\n",
    "        \"batch_size_\": {\"values\": [32]},\n",
    "        \"epochs_\": {\"values\": [15]},\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep inizialization\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"provaA1\")\n",
    "\n",
    "# sweep execution\n",
    "wandb.agent(sweep_id, function=train_and_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live emotion detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "model = EmotionCNN(num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(\"../../../Models/paper1_models/best_model_paper_1_20_epochs_bs_48_30k.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# initialize the face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# apply the transformations to the face image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # faces detection\n",
    "    faces = detector(frame)\n",
    "\n",
    "    # if there is at least one face detected, process the image\n",
    "    if len(faces) > 0:\n",
    "        # take only the first face\n",
    "        face = faces[0]\n",
    "        \n",
    "        # cut the face from the frame\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # check if the face image is not empty\n",
    "        if not face_image.size == 0:\n",
    "            # apply the transformations to the face image\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "            input_image = transform(pil_image).unsqueeze(0)  # Aggiunge una dimensione di batch\n",
    "            input_image = input_image.to(device)\n",
    "\n",
    "            # model prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_image)\n",
    "\n",
    "            # get the label predicted by the model\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_emotion = your_label_mapping[predicted.item()]\n",
    "\n",
    "            print(f'Predicted Emotion: {predicted_emotion}')\n",
    "\n",
    "    # show the frame with the face rectangle added\n",
    "    cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "    # wait for 2 seconds (time in milliseconds)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    # if q is pressed, terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
