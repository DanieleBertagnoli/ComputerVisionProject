{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WINDOWS (your env must be called project-venv; if you choose another name add it in .gitignore)\n",
    "import subprocess\n",
    "\n",
    "# Set the execution policy\n",
    "subprocess.run([\"Set-ExecutionPolicy\", \"RemoteSigned\", \"-Scope\", \"Process\"], shell=True)\n",
    "\n",
    "# Activate the virtual environment\n",
    "subprocess.run([\"cd\", \".\\\\project-venv\\\\\"], shell=True)\n",
    "subprocess.run([\".\\\\Scripts\\\\Activate.ps1\"], shell=True)\n",
    "\n",
    "# Install requirements\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../requirements.txt\"], shell=True)\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../emotions_requirements.txt\"], shell=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Emotion Recognition task**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import dlib\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wandb login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='d29d51017f4231b5149d36ad242526b374c9c60a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Paper implementation 1**\n",
    "https://ieeexplore.ieee.org/abstract/document/9659697?casa_token=zDD7lwwOig8AAAAA:KcIHhupXAXgiaB_C7A0uNDB7ehrsWNyovQdgDu9LmnwToOGU6akB_gjWTy7JCf4UdKK03Is\n",
    "\n",
    "https://github.com/serengil/deepface"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Dataset Augmenting**\n",
    "\n",
    "#### **Objective:**\n",
    "The purpose of this Python script is to perform data augmentation on a dataset of images corresponding to different emotions. Data augmentation is a technique commonly used in computer vision tasks to increase the diversity of the training dataset, thereby enhancing the robustness and generalization capabilities of machine learning models.\n",
    "\n",
    "#### **Dataset Structure:**\n",
    "The original dataset is organized into folders, each representing a specific emotion (e.g., angry, disgust, fear, happy, neutral, sad, surprise). Each emotion folder contains a collection of images in formats such as JPEG and PNG.\n",
    "\n",
    "#### **Transformation Techniques:**\n",
    "The script employs various image transformation techniques to augment the dataset. These techniques include:\n",
    "\n",
    "1. **Horizontal Flip:** Flips the image horizontally.\n",
    "2. **Vertical Flip:** Flips the image vertically.\n",
    "3. **Zoom:** Randomly zooms into or out of the image.\n",
    "4. **Translation:** Shifts the image horizontally and vertically.\n",
    "5. **Contrast and Brightness Adjustment:** Randomly adjusts the contrast and brightness of the image.\n",
    "6. **Elastic Transformation:** Applies a non-linear elastic deformation to the image.\n",
    "\n",
    "#### **Implementation:**\n",
    "The script uses the OpenCV library for image processing. For each emotion category, it iterates through the images, applies the defined transformations using the `apply_transformations` function, and saves the augmented images in a new folder named with the emotion followed by \"_augmented.\"\n",
    "\n",
    "The `elastic_transform` function generates an elastic transformation by displacing pixels based on random elastic deformations. This adds a degree of distortion to the images, contributing to further variability.\n",
    "\n",
    "#### **Output:**\n",
    "The augmented images are saved in the corresponding emotion's augmented folder, with filenames indicating the applied augmentation technique. For example, an image originally named \"example_image.jpg\" may result in augmented images like \"example_image_aug_0.jpg,\" \"example_image_aug_1.jpg,\" etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# cycle through emotions\n",
    "for emotion in emotions:\n",
    "    # path of the folder containing the images\n",
    "    folder_path = fr\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\\{emotion}\"\n",
    "    output_folder_path = fr\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\\{emotion}_augmented\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # list of images in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # define transformations inside the apply_transformations function\n",
    "    def apply_transformations(image):\n",
    "        # horizontal_flip\n",
    "        flipped_horizontal = cv2.flip(image, 1)\n",
    "\n",
    "        # vertical flip\n",
    "        flipped_vertical = cv2.flip(image, 0)\n",
    "\n",
    "        # Zoom\n",
    "        zoom_factor = random.uniform(0.8, 1.2)\n",
    "        height, width = image.shape[:2]\n",
    "        zoomed_image = cv2.resize(image, (int(width * zoom_factor), int(height * zoom_factor)))\n",
    "\n",
    "        # translation\n",
    "        tx = random.randint(-10, 10)\n",
    "        ty = random.randint(-10, 10)\n",
    "        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        translated_image = cv2.warpAffine(image, translation_matrix, (width, height))\n",
    "\n",
    "        # contrast and brightness control\n",
    "        alpha = random.uniform(0.8, 1.2)\n",
    "        beta = random.randint(-35, 35)\n",
    "        adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "        # elastic transformation\n",
    "        elastic_image = elastic_transform(image, alpha=random.randint(6, 14), sigma=random.uniform(1.1, 2.0))\n",
    "\n",
    "        return [image, translated_image, flipped_horizontal, zoomed_image, adjusted_image, elastic_image] #forse togliere flipped vertical\n",
    "\n",
    "    def elastic_transform(image, alpha, sigma):\n",
    "        random_state = np.random.RandomState(None)\n",
    "        shape = image.shape\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dz = np.zeros_like(dx)\n",
    "\n",
    "        x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "        indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "\n",
    "        distorted_image = map_coordinates(image, indices, order=1, mode='reflect')\n",
    "        distorted_image = distorted_image.reshape(image.shape)\n",
    "\n",
    "        return distorted_image\n",
    "\n",
    "    # apply data augmentation\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        augmented_images = apply_transformations(image)\n",
    "\n",
    "        # save new images\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        for i, augmented_image in enumerate(augmented_images):\n",
    "            output_file_path = os.path.join(output_folder_path, f\"{base_name}_aug_{i}.jpg\")\n",
    "            cv2.imwrite(output_file_path, augmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **CNN Architecture: EmotionCNN**\n",
    "\n",
    "#### **Overview:**\n",
    "The `EmotionCNN` model is designed for emotion classification using a Convolutional Neural Network (CNN). Below is an overview of its architecture:\n",
    "\n",
    "#### **Architecture Layers:**\n",
    "\n",
    "1. **Input Layer:**\n",
    "   - Input channels: 1 (grayscale images)\n",
    "   - Input size: Variable (depends on the input image size)\n",
    "\n",
    "2. **Convolutional Layer 1:**\n",
    "   - Convolutional operation: 64 filters of size 5x5\n",
    "   - Activation function: ReLU\n",
    "   - Max pooling: 5x5 kernel with stride 2\n",
    "\n",
    "3. **Convolutional Layers 2a and 2b:**\n",
    "   - Convolutional operation: 64 filters of size 3x3\n",
    "   - Activation functions: ReLU for both layers\n",
    "   - Average pooling: 3x3 kernel with stride 2\n",
    "\n",
    "4. **Convolutional Layers 3a and 3b:**\n",
    "   - Convolutional operation: 128 filters of size 3x3\n",
    "   - Activation functions: ReLU for both layers\n",
    "   - Average pooling: 3x3 kernel with stride 2\n",
    "\n",
    "5. **Fully Connected (FC) Layer 1:**\n",
    "   - Units: 1024\n",
    "   - Activation function: ReLU\n",
    "   - Dropout: 20% dropout rate\n",
    "\n",
    "6. **Fully Connected (FC) Layer 2:**\n",
    "   - Units: 1024\n",
    "   - Activation function: ReLU\n",
    "   - Dropout: 20% dropout rate\n",
    "\n",
    "7. **Output Layer:**\n",
    "   - Units: Variable (depends on the number of emotion classes)\n",
    "   - Activation function: Softmax\n",
    "\n",
    "#### **Additional Information:**\n",
    "- The architecture is designed for grayscale images (1 channel).\n",
    "- The network utilizes ReLU activation functions after convolutional and fully connected layers.\n",
    "- Max pooling and average pooling layers are employed for down-sampling.\n",
    "- Dropout layers (20% dropout rate) are included for regularization.\n",
    "- The final layer uses the softmax activation for multiclass classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2a = nn.ReLU()\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2b = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3a = nn.ReLU()\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3b = nn.ReLU()\n",
    "        self.avgpool3 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # verify the output size of conv2 and conv3\n",
    "        self.dummy_input = torch.randn(1, 1, 48, 48)\n",
    "        self.dummy_output_size = self._get_conv_output_size(self.dummy_input)\n",
    "        \n",
    "        # update fc1 units based on feature map size\n",
    "        self.fc1 = nn.Linear(self.dummy_output_size, 1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output_size(self, input_tensor):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(input_tensor)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(self.relu_fc1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu_fc2(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Hyperparameters**\n",
    "\n",
    "The parameters that can be modified before performing training are the number of instances per label, the batch size and the number of epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_instances_over_under_sampling_ = 30000\n",
    "batch_size_ = 58\n",
    "epochs_ = 50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Delete Outliers with DBSCAN**\n",
    "\n",
    "#### **Objective:**\n",
    "The Python script is designed to identify and remove outliers from a collection of images using the Density-Based Spatial Clustering of Applications with Noise (DBSCAN) algorithm. The primary goal is to filter out images with unusual pixel statistics, specifically focusing on standard deviation.\n",
    "\n",
    "#### **Functions:**\n",
    "\n",
    "1. **`calculate_pixel_std(image_path):`**\n",
    "   - **Input:** Path to an image file.\n",
    "   - **Output:** Returns the standard deviation of pixel values in the grayscale image.\n",
    "\n",
    "2. **`remove_outliers_dbscan(folder_path, eps, min_samples):`**\n",
    "   - **Input:**\n",
    "     - `folder_path`: Path to the folder containing images.\n",
    "     - `eps`: DBSCAN search radius parameter.\n",
    "     - `min_samples`: Minimum number of samples required for a cluster in DBSCAN.\n",
    "   - **Action:**\n",
    "     - Calculates the pixel standard deviation for each image in the folder.\n",
    "     - Applies DBSCAN to cluster images based on pixel standard deviation.\n",
    "     - Removes outliers (images with label -1) by deleting them from the folder.\n",
    "   - **Output:** Prints the number of outliers removed.\n",
    "\n",
    "#### **Usage:**\n",
    "\n",
    "1. Set the DBSCAN configuration parameters:\n",
    "   - `dbscan_eps`: DBSCAN search radius.\n",
    "   - `dbscan_min_samples`: Minimum number of samples required for a cluster.\n",
    "\n",
    "2. Provide the path to the folder containing emotion-specific image folders (`emotions_folder_path`).\n",
    "\n",
    "3. Iterate through each emotion folder:\n",
    "   - If the emotion is 'disgust,' use different DBSCAN parameters.\n",
    "   - Apply `remove_outliers_dbscan` to delete outliers in each emotion folder.\n",
    "\n",
    "4. The script prints the number of outliers removed for each emotion."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def remove_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # remove outliers\n",
    "    counter = 0\n",
    "    for i, (label, image) in enumerate(zip(labels, os.listdir(folder_path))):\n",
    "        if label == -1:  \n",
    "            image_path = os.path.join(folder_path, image)\n",
    "            os.remove(image_path)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "# DBSCAN configuration\n",
    "dbscan_eps = 0.4  # search radius\n",
    "dbscan_min_samples = 15  # minimum number of samples required for a cluster\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\"\n",
    "\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "    \n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        remove_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        remove_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the path and the number of the outliers per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "angry\n",
      "Label -1: 44 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_24139016_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_27212219_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_33469617_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_51610150_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_66705645_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_66705645_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_67702983_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PrivateTest_68333170_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\PublicTest_48413488_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_17526342_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_19726930_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_25131560_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_3074482_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_3279666_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_35803006_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_3598867_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_37911499_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_40192268_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_5300834_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_56096400_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_5630225_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_5645946_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_5645946_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_57549098_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61077408_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_61773462_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_63604344_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_65978744_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_6644670_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_67604214_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_71571159_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_7206687_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_72188974_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_73598349_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_87875595_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_94196223_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_95575116_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_96216570_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\angry\\Training_99588209_aug_5.jpg\n",
      "disgust\n",
      "Label -1: 47 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PrivateTest_34013087_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PrivateTest_53414692_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PrivateTest_76656607_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PrivateTest_98799539_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PublicTest_11387162_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PublicTest_41633403_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PublicTest_89570683_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\PublicTest_98815442_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_1070239_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_12026955_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_14719775_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_15104479_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_17705855_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_30792119_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_31108028_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_31108028_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_31573701_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_36703585_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_41001987_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_41238958_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_44470373_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_44993228_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_5258769_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_55559187_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_57692374_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_59161460_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_59409463_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_61291772_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_61342142_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_62239274_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_62239274_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_62239274_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_62886279_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_63147438_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_63156802_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_65759222_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_67030775_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_71118638_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_71679475_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_73889032_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_75609687_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_79212194_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_82237730_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_85767348_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_93689671_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_96754089_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\disgust\\Training_9948239_aug_5.jpg\n",
      "fear\n",
      "Label -1: 65 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_10306709_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_26581691_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_28273224_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_36996738_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_36996738_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_36996738_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PrivateTest_89129667_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PublicTest_15468267_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\PublicTest_54848341_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_1018372_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_10942406_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_13181629_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_18899448_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_2161263_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_21644135_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_24058572_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_25561886_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_26823187_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_30471858_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_31107495_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_32346910_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_32735149_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_33075662_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_33131991_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_34578336_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35100459_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_35633778_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_38188558_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_45169609_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_45488765_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_46366812_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_50362368_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_54053151_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_61960556_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_62764355_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_630805_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_67109543_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_72328939_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_73966221_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_75260133_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_75993265_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_81410671_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_81474503_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_81525924_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_81755794_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_84303199_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_87394233_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_8752857_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_9014912_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_93325481_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_93583767_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_93766134_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_9428052_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_95161986_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\fear\\Training_98882902_aug_6.jpg\n",
      "happy\n",
      "Label -1: 47 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PrivateTest_35915421_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PrivateTest_5142883_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_21832858_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_40705304_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_81760346_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_8909872_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\PublicTest_89636551_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_11546938_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_13969968_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_19307133_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_26537121_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_29662843_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_33313497_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_35439889_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_3720083_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_39657861_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_3984085_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_40433185_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_51289797_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_62100199_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_62231267_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_65883318_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_66991991_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_79076701_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_86210783_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_87607167_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_88157591_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_9961393_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\happy\\Training_99906271_aug_5.jpg\n",
      "neutral\n",
      "Label -1: 55 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PrivateTest_57076415_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PrivateTest_57076415_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PrivateTest_69339694_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PrivateTest_9130658_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_22397103_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_69096583_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_85599431_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_85967214_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_86444127_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_98333211_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_98333211_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_98333211_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\PublicTest_98333211_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_15913635_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_17843961_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_20906549_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_21438500_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_23468066_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_24022585_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_2445425_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_31414356_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_3542668_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_36986471_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_37024304_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_38562405_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_42279539_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_51124890_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_5177012_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_54750821_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_55091472_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_6110388_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_63359167_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_63754753_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_66880865_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_75186221_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_7749247_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_78787380_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_85413312_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_89335926_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_92030023_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_92347458_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_96958424_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_97267200_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\neutral\\Training_98951914_aug_1.jpg\n",
      "sad\n",
      "Label -1: 51 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_13548922_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_22791863_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_43570474_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_51713257_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_63060469_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_70717012_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PrivateTest_82792706_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PublicTest_19663674_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PublicTest_83014731_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\PublicTest_97750311_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_11036409_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_11175351_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_12165608_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_20336575_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_22912966_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25721710_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25949094_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25949094_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25949094_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25949094_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_25949094_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_35332527_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_35332527_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_35332527_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_35332527_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_48621797_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_50699732_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_594426_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_63208278_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_6436858_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_65552938_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_66106251_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_67842276_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_68289103_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_76656854_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_77117966_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_7816570_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_8214295_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_87857337_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_87857337_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_87857337_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_87857337_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_87857337_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_91805461_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\sad\\Training_96421728_aug_5.jpg\n",
      "surprise\n",
      "Label -1: 41 outliers\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_21197746_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_23514058_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_33008622_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_35458201_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_49077480_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_66164599_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_73154398_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PrivateTest_97163676_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PublicTest_39373170_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PublicTest_78591767_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PublicTest_80135522_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\PublicTest_83394959_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_10264146_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_12232006_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_27672614_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_27672614_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_29567186_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_42143192_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_42143192_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_0.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_2.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_3.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_4.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_48403842_aug_6.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_49076622_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_50140086_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_57669898_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_6966074_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_72825602_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_73838593_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_75067608_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_76252560_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_79616853_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_79769703_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_84259888_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_86145739_aug_1.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_86145739_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_90167636_aug_5.jpg\n",
      "C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\\surprise\\Training_96572434_aug_5.jpg\n"
     ]
    }
   ],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def get_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # collect outlier paths and count for each label\n",
    "    outlier_paths_by_label = {}\n",
    "    for label, image_path in zip(labels, image_paths):\n",
    "        if label == -1:\n",
    "            if label not in outlier_paths_by_label:\n",
    "                outlier_paths_by_label[label] = []\n",
    "            outlier_paths_by_label[label].append(image_path)\n",
    "\n",
    "    return outlier_paths_by_label\n",
    "\n",
    "# Configurazione DBSCAN\n",
    "dbscan_eps = 0.5  # Raggio di ricerca\n",
    "dbscan_min_samples = 10  # Numero minimo di campioni in un cluster\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\"\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "\n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        outliers = get_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        outliers = get_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n",
    "\n",
    "    # Stampa i percorsi degli outliers per ogni label\n",
    "    for label, outlier_paths in outliers.items():\n",
    "        print(f\"Label {label}: {len(outlier_paths)} outliers\")\n",
    "        for path in outlier_paths:\n",
    "            print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Over and Under sampling: Class balance**\n",
    "We apply the transformations on the image dataset: grayscale, resize, conversion to tensor and normalization . Weighted random samplers handle class imbalances."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation definition\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images'\n",
    "\n",
    "# create an instance of ImageFolder with the transformations\n",
    "dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "# seed = 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# extract the labels and the indices of the dataset\n",
    "labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "# convert the list into a tensor\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# calculate the number of instances for each class\n",
    "counts = torch.bincount(labels)\n",
    "\n",
    "# calculate the weights for each class\n",
    "weights = 1.0 / counts.float()\n",
    "\n",
    "# create a weight vector for each index in the dataset\n",
    "sample_weights = weights[labels]\n",
    "\n",
    "# set the number of samples for the train set and the test set\n",
    "train_size = number_instances_over_under_sampling_ * 7 * 0.8\n",
    "val_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "test_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "\n",
    "# crea un sampler per il train set and one for the test set\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "# create a dataloader for the train set and the test set with the corresponding samplers\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifying if for each label there are number_instances_over_under_sampling instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of instances for class in the train set: tensor([16164, 15870, 16006, 15886, 15980, 15859, 16235])\n",
      "number of instances for class in the validation set: tensor([2017, 2013, 1980, 2019, 1985, 1996, 1990])\n",
      "number of instances for class in the test set: tensor([2029, 2039, 1972, 1939, 2043, 2019, 1959])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_13568\\60523432.py:2: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_counts = torch.bincount(torch.tensor(labels)[train_indices])\n",
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_13568\\60523432.py:6: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  val_counts = torch.bincount(torch.tensor(labels)[val_indices])\n",
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_13568\\60523432.py:10: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_counts = torch.bincount(torch.tensor(labels)[test_indices])\n"
     ]
    }
   ],
   "source": [
    "train_indices = list(train_loader.sampler)\n",
    "train_counts = torch.bincount(torch.tensor(labels)[train_indices])\n",
    "print(\"number of instances for class in the train set:\", train_counts)\n",
    "\n",
    "val_indices = list(val_loader.sampler)\n",
    "val_counts = torch.bincount(torch.tensor(labels)[val_indices])\n",
    "print(\"number of instances for class in the validation set:\", val_counts)\n",
    "\n",
    "test_indices = list(test_loader.sampler)\n",
    "test_counts = torch.bincount(torch.tensor(labels)[test_indices])\n",
    "print(\"number of instances for class in the test set:\", test_counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(3500, 438, 438)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(val_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning these are the number of instances for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class angry_augmented: 29680 istances\n",
      "Class disgust_augmented: 3216 istances\n",
      "Class fear_augmented: 30675 istances\n",
      "Class happy_augmented: 53867 istances\n",
      "Class neutral_augmented: 37132 istances\n",
      "Class sad_augmented: 36418 istances\n",
      "Class surprise_augmented: 23981 istances\n"
     ]
    }
   ],
   "source": [
    "# obtain the classes (labels)\n",
    "classes = dataset.classes\n",
    "\n",
    "# count the instances for each class\n",
    "instances_per_class = {cls: 0 for cls in classes}\n",
    "\n",
    "for _, label in dataset.imgs:\n",
    "    instances_per_class[classes[label]] += 1\n",
    "\n",
    "# print(\"Number of instances per class:\")\n",
    "for cls, count in instances_per_class.items():\n",
    "    print(f\"Class {cls}: {count} istances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Emotion Recognition Training and Evaluation**\n",
    "\n",
    "This code performs emotion recognition using a Convolutional Neural Network (CNN). Key components include:\n",
    "\n",
    "- **Emotion Label Mapping**: Emotions are mapped to numerical values for training and evaluation.\n",
    "- **Model Definition and Setup**: A CNN model, along with the criterion, optimizer, and scheduler, is defined.\n",
    "- **Metrics Calculation**: The `calculate_metrics_per_class` function computes accuracy, precision, recall, F1 score, and support for each class.\n",
    "- **Training Function**: The `train_epoch` function trains the model for one epoch, logging loss, accuracy, and metrics per class.\n",
    "- **Evaluation Function**: The `evaluate` function assesses the model on the validation set, calculating loss, accuracy, and metrics per class.\n",
    "- **Test Function**: The `test` function evaluates the model on the test set, providing loss, accuracy, and metrics per class.\n",
    "- **Training and Evaluation Process**: The `train_and_evaluate` function orchestrates the entire process. It initializes WandB, sets up data transformations, creates weighted random samplers for handling class imbalances, and conducts training and validation. The best model is saved, and evaluation metrics are logged.\n",
    "- **Hyperparameter Sweep Configuration**: The code supports hyperparameter optimization using WandB sweeps. Parameters include the number of instances for over/under-sampling, batch size, and epochs.\n",
    "\n",
    "#### Training Process\n",
    "\n",
    "The training process involves iterating through epochs, training on the specified dataset, and evaluating on validation data. Early stopping is implemented to prevent overfitting.\n",
    "\n",
    "#### Logging and Visualization\n",
    "\n",
    "WandB is integrated for experiment tracking. Metrics such as loss, accuracy, and metrics per class are logged during training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Training:   0%|          | 0/3500 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\sklearn\\metrics\\_classification.py:1471: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
      "  _warn_prf(average, modifier, msg_start, len(result))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/15 => Train Loss: 1.9459, Train Accuracy: 0.1420\n",
      "Angry: Train Precision: 0.1567, Train Recall: 0.0013, Train F1: 0.0027, Train Support: 15711\n",
      "Disgust: Train Precision: 0.1458, Train Recall: 0.1059, Train F1: 0.1227, Train Support: 16118\n",
      "Fear: Train Precision: 0.1425, Train Recall: 0.0939, Train F1: 0.1132, Train Support: 16068\n",
      "Happy: Train Precision: 0.1417, Train Recall: 0.1087, Train F1: 0.1230, Train Support: 16028\n",
      "Neutral: Train Precision: 0.1418, Train Recall: 0.6468, Train F1: 0.2326, Train Support: 16138\n",
      "Sad: Train Precision: 0.1274, Train Recall: 0.0281, Train F1: 0.0460, Train Support: 15952\n",
      "Surprise: Train Precision: 0.2635, Train Recall: 0.0024, Train F1: 0.0048, Train Support: 15985\n",
      "Validation Loss: 1.9457, Validation Accuracy: 0.1688\n",
      "Angry: Validation Precision: 0.0000, Validation Recall: 0.0000, Validation F1: 0.0000, Validation Support: 2003\n",
      "Disgust: Validation Precision: 0.1645, Validation Recall: 0.1707, Validation F1: 0.1675, Validation Support: 1980\n",
      "Fear: Validation Precision: 0.2727, Validation Recall: 0.0015, Validation F1: 0.0030, Validation Support: 1983\n",
      "Happy: Validation Precision: 0.0000, Validation Recall: 0.0000, Validation F1: 0.0000, Validation Support: 1966\n",
      "Neutral: Validation Precision: 0.1533, Validation Recall: 0.7500, Validation F1: 0.2546, Validation Support: 2044\n",
      "Sad: Validation Precision: 0.0000, Validation Recall: 0.0000, Validation F1: 0.0000, Validation Support: 2021\n",
      "Surprise: Validation Precision: 0.2527, Validation Recall: 0.2441, Validation F1: 0.2483, Validation Support: 2003\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 2/15 => Train Loss: 1.9188, Train Accuracy: 0.2017\n",
      "Angry: Train Precision: 0.1950, Train Recall: 0.1197, Train F1: 0.1484, Train Support: 15929\n",
      "Disgust: Train Precision: 0.1736, Train Recall: 0.1485, Train F1: 0.1601, Train Support: 15924\n",
      "Fear: Train Precision: 0.1451, Train Recall: 0.1201, Train F1: 0.1314, Train Support: 16058\n",
      "Happy: Train Precision: 0.2225, Train Recall: 0.0647, Train F1: 0.1003, Train Support: 15850\n",
      "Neutral: Train Precision: 0.1861, Train Recall: 0.1319, Train F1: 0.1544, Train Support: 16027\n",
      "Sad: Train Precision: 0.1897, Train Recall: 0.2882, Train F1: 0.2288, Train Support: 16097\n",
      "Surprise: Train Precision: 0.2468, Train Recall: 0.5341, Train F1: 0.3376, Train Support: 16115\n",
      "Validation Loss: 1.8791, Validation Accuracy: 0.2562\n",
      "Angry: Validation Precision: 0.3750, Validation Recall: 0.0015, Validation F1: 0.0030, Validation Support: 1975\n",
      "Disgust: Validation Precision: 0.2505, Validation Recall: 0.2746, Validation F1: 0.2620, Validation Support: 1919\n",
      "Fear: Validation Precision: 0.2459, Validation Recall: 0.0074, Validation F1: 0.0144, Validation Support: 2026\n",
      "Happy: Validation Precision: 0.2516, Validation Recall: 0.3724, Validation F1: 0.3003, Validation Support: 2046\n",
      "Neutral: Validation Precision: 0.3500, Validation Recall: 0.0035, Validation F1: 0.0070, Validation Support: 1980\n",
      "Sad: Validation Precision: 0.1877, Validation Recall: 0.5643, Validation F1: 0.2816, Validation Support: 1999\n",
      "Surprise: Validation Precision: 0.4138, Validation Recall: 0.5572, Validation F1: 0.4749, Validation Support: 2055\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 3/15 => Train Loss: 1.8389, Train Accuracy: 0.3052\n",
      "Angry: Train Precision: 0.2293, Train Recall: 0.1548, Train F1: 0.1848, Train Support: 15895\n",
      "Disgust: Train Precision: 0.3283, Train Recall: 0.4308, Train F1: 0.3726, Train Support: 16232\n",
      "Fear: Train Precision: 0.1714, Train Recall: 0.0354, Train F1: 0.0587, Train Support: 15762\n",
      "Happy: Train Precision: 0.3243, Train Recall: 0.4243, Train F1: 0.3676, Train Support: 16056\n",
      "Neutral: Train Precision: 0.2549, Train Recall: 0.2622, Train F1: 0.2585, Train Support: 16064\n",
      "Sad: Train Precision: 0.2268, Train Recall: 0.2193, Train F1: 0.2230, Train Support: 16094\n",
      "Surprise: Train Precision: 0.4071, Train Recall: 0.6050, Train F1: 0.4867, Train Support: 15897\n",
      "Validation Loss: 1.7902, Validation Accuracy: 0.3614\n",
      "Angry: Validation Precision: 0.2616, Validation Recall: 0.2103, Validation F1: 0.2332, Validation Support: 1978\n",
      "Disgust: Validation Precision: 0.4702, Validation Recall: 0.5153, Validation F1: 0.4917, Validation Support: 2053\n",
      "Fear: Validation Precision: 0.1889, Validation Recall: 0.1000, Validation F1: 0.1308, Validation Support: 2010\n",
      "Happy: Validation Precision: 0.3776, Validation Recall: 0.5490, Validation F1: 0.4474, Validation Support: 1969\n",
      "Neutral: Validation Precision: 0.2874, Validation Recall: 0.5035, Validation F1: 0.3659, Validation Support: 2016\n",
      "Sad: Validation Precision: 0.2732, Validation Recall: 0.0850, Validation F1: 0.1296, Validation Support: 2012\n",
      "Surprise: Validation Precision: 0.5383, Validation Recall: 0.5693, Validation F1: 0.5534, Validation Support: 1962\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 4/15 => Train Loss: 1.7769, Train Accuracy: 0.3736\n",
      "Angry: Train Precision: 0.2833, Train Recall: 0.1827, Train F1: 0.2221, Train Support: 15989\n",
      "Disgust: Train Precision: 0.4272, Train Recall: 0.5673, Train F1: 0.4874, Train Support: 16133\n",
      "Fear: Train Precision: 0.2142, Train Recall: 0.0607, Train F1: 0.0946, Train Support: 16103\n",
      "Happy: Train Precision: 0.4249, Train Recall: 0.5195, Train F1: 0.4675, Train Support: 15895\n",
      "Neutral: Train Precision: 0.3149, Train Recall: 0.3918, Train F1: 0.3492, Train Support: 15861\n",
      "Sad: Train Precision: 0.2555, Train Recall: 0.2465, Train F1: 0.2509, Train Support: 15867\n",
      "Surprise: Train Precision: 0.4903, Train Recall: 0.6444, Train F1: 0.5569, Train Support: 16152\n",
      "Validation Loss: 1.7598, Validation Accuracy: 0.3937\n",
      "Angry: Validation Precision: 0.2871, Validation Recall: 0.3041, Validation F1: 0.2953, Validation Support: 1993\n",
      "Disgust: Validation Precision: 0.4807, Validation Recall: 0.5772, Validation F1: 0.5245, Validation Support: 2027\n",
      "Fear: Validation Precision: 0.2186, Validation Recall: 0.1208, Validation F1: 0.1556, Validation Support: 1945\n",
      "Happy: Validation Precision: 0.5720, Validation Recall: 0.4293, Validation F1: 0.4905, Validation Support: 2064\n",
      "Neutral: Validation Precision: 0.4287, Validation Recall: 0.2407, Validation F1: 0.3083, Validation Support: 1986\n",
      "Sad: Validation Precision: 0.3078, Validation Recall: 0.3354, Validation F1: 0.3210, Validation Support: 2072\n",
      "Surprise: Validation Precision: 0.4170, Validation Recall: 0.7538, Validation F1: 0.5370, Validation Support: 1913\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5/15 => Train Loss: 1.7310, Train Accuracy: 0.4226\n",
      "Angry: Train Precision: 0.3374, Train Recall: 0.2533, Train F1: 0.2894, Train Support: 16061\n",
      "Disgust: Train Precision: 0.5076, Train Recall: 0.5953, Train F1: 0.5479, Train Support: 16005\n",
      "Fear: Train Precision: 0.2498, Train Recall: 0.0663, Train F1: 0.1048, Train Support: 15849\n",
      "Happy: Train Precision: 0.4830, Train Recall: 0.6139, Train F1: 0.5407, Train Support: 16132\n",
      "Neutral: Train Precision: 0.3531, Train Recall: 0.4481, Train F1: 0.3950, Train Support: 16055\n",
      "Sad: Train Precision: 0.2902, Train Recall: 0.2838, Train F1: 0.2870, Train Support: 15869\n",
      "Surprise: Train Precision: 0.5389, Train Recall: 0.6912, Train F1: 0.6056, Train Support: 16029\n",
      "Validation Loss: 1.7261, Validation Accuracy: 0.4256\n",
      "Angry: Validation Precision: 0.4328, Validation Recall: 0.1883, Validation F1: 0.2624, Validation Support: 2002\n",
      "Disgust: Validation Precision: 0.4790, Validation Recall: 0.6549, Validation F1: 0.5533, Validation Support: 2005\n",
      "Fear: Validation Precision: 0.2891, Validation Recall: 0.1052, Validation F1: 0.1543, Validation Support: 1986\n",
      "Happy: Validation Precision: 0.5748, Validation Recall: 0.6028, Validation F1: 0.5885, Validation Support: 2014\n",
      "Neutral: Validation Precision: 0.3331, Validation Recall: 0.5017, Validation F1: 0.4004, Validation Support: 2047\n",
      "Sad: Validation Precision: 0.2898, Validation Recall: 0.4740, Validation F1: 0.3597, Validation Support: 2038\n",
      "Surprise: Validation Precision: 0.7502, Validation Recall: 0.4471, Validation F1: 0.5603, Validation Support: 1908\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 6/15 => Train Loss: 1.6961, Train Accuracy: 0.4604\n",
      "Angry: Train Precision: 0.3728, Train Recall: 0.3102, Train F1: 0.3387, Train Support: 15995\n",
      "Disgust: Train Precision: 0.5681, Train Recall: 0.6318, Train F1: 0.5982, Train Support: 16029\n",
      "Fear: Train Precision: 0.3195, Train Recall: 0.1087, Train F1: 0.1622, Train Support: 15921\n",
      "Happy: Train Precision: 0.5400, Train Recall: 0.6344, Train F1: 0.5834, Train Support: 16091\n",
      "Neutral: Train Precision: 0.3794, Train Recall: 0.4683, Train F1: 0.4192, Train Support: 15966\n",
      "Sad: Train Precision: 0.3281, Train Recall: 0.3584, Train F1: 0.3426, Train Support: 16132\n",
      "Surprise: Train Precision: 0.5869, Train Recall: 0.7106, Train F1: 0.6429, Train Support: 15866\n",
      "Validation Loss: 1.7015, Validation Accuracy: 0.4565\n",
      "Angry: Validation Precision: 0.4024, Validation Recall: 0.3555, Validation F1: 0.3775, Validation Support: 1983\n",
      "Disgust: Validation Precision: 0.6164, Validation Recall: 0.6431, Validation F1: 0.6295, Validation Support: 2026\n",
      "Fear: Validation Precision: 0.4938, Validation Recall: 0.0405, Validation F1: 0.0749, Validation Support: 1974\n",
      "Happy: Validation Precision: 0.3737, Validation Recall: 0.8392, Validation F1: 0.5171, Validation Support: 2027\n",
      "Neutral: Validation Precision: 0.3492, Validation Recall: 0.5332, Validation F1: 0.4220, Validation Support: 1943\n",
      "Sad: Validation Precision: 0.4898, Validation Recall: 0.1398, Validation F1: 0.2175, Validation Support: 2067\n",
      "Surprise: Validation Precision: 0.6855, Validation Recall: 0.6449, Validation F1: 0.6646, Validation Support: 1980\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 7/15 => Train Loss: 1.6679, Train Accuracy: 0.4885\n",
      "Angry: Train Precision: 0.4054, Train Recall: 0.3555, Train F1: 0.3788, Train Support: 15938\n",
      "Disgust: Train Precision: 0.6185, Train Recall: 0.6825, Train F1: 0.6489, Train Support: 15892\n",
      "Fear: Train Precision: 0.3372, Train Recall: 0.1417, Train F1: 0.1996, Train Support: 15937\n",
      "Happy: Train Precision: 0.5781, Train Recall: 0.6525, Train F1: 0.6131, Train Support: 16179\n",
      "Neutral: Train Precision: 0.4009, Train Recall: 0.4898, Train F1: 0.4409, Train Support: 16115\n",
      "Sad: Train Precision: 0.3526, Train Recall: 0.3798, Train F1: 0.3657, Train Support: 16064\n",
      "Surprise: Train Precision: 0.6147, Train Recall: 0.7178, Train F1: 0.6623, Train Support: 15875\n",
      "Validation Loss: 1.6442, Validation Accuracy: 0.5146\n",
      "Angry: Validation Precision: 0.4457, Validation Recall: 0.3763, Validation F1: 0.4081, Validation Support: 1961\n",
      "Disgust: Validation Precision: 0.7029, Validation Recall: 0.7094, Validation F1: 0.7061, Validation Support: 2034\n",
      "Fear: Validation Precision: 0.4515, Validation Recall: 0.0912, Validation F1: 0.1518, Validation Support: 2039\n",
      "Happy: Validation Precision: 0.6210, Validation Recall: 0.6568, Validation F1: 0.6384, Validation Support: 1961\n",
      "Neutral: Validation Precision: 0.3802, Validation Recall: 0.6298, Validation F1: 0.4741, Validation Support: 2053\n",
      "Sad: Validation Precision: 0.4213, Validation Recall: 0.3694, Validation F1: 0.3936, Validation Support: 2079\n",
      "Surprise: Validation Precision: 0.5765, Validation Recall: 0.7944, Validation F1: 0.6682, Validation Support: 1873\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 8/15 => Train Loss: 1.6424, Train Accuracy: 0.5165\n",
      "Angry: Train Precision: 0.4434, Train Recall: 0.3811, Train F1: 0.4099, Train Support: 15974\n",
      "Disgust: Train Precision: 0.6659, Train Recall: 0.7160, Train F1: 0.6901, Train Support: 16004\n",
      "Fear: Train Precision: 0.3769, Train Recall: 0.1745, Train F1: 0.2386, Train Support: 15977\n",
      "Happy: Train Precision: 0.5992, Train Recall: 0.6889, Train F1: 0.6409, Train Support: 16102\n",
      "Neutral: Train Precision: 0.4206, Train Recall: 0.5077, Train F1: 0.4601, Train Support: 15934\n",
      "Sad: Train Precision: 0.3737, Train Recall: 0.4107, Train F1: 0.3913, Train Support: 15993\n",
      "Surprise: Train Precision: 0.6410, Train Recall: 0.7343, Train F1: 0.6845, Train Support: 16016\n",
      "Validation Loss: 1.6301, Validation Accuracy: 0.5293\n",
      "Angry: Validation Precision: 0.4929, Validation Recall: 0.3563, Validation F1: 0.4136, Validation Support: 1962\n",
      "Disgust: Validation Precision: 0.7196, Validation Recall: 0.7752, Validation F1: 0.7464, Validation Support: 1953\n",
      "Fear: Validation Precision: 0.3799, Validation Recall: 0.1418, Validation F1: 0.2065, Validation Support: 1975\n",
      "Happy: Validation Precision: 0.6056, Validation Recall: 0.7288, Validation F1: 0.6615, Validation Support: 1995\n",
      "Neutral: Validation Precision: 0.4392, Validation Recall: 0.5174, Validation F1: 0.4751, Validation Support: 2037\n",
      "Sad: Validation Precision: 0.4459, Validation Recall: 0.3510, Validation F1: 0.3928, Validation Support: 2080\n",
      "Surprise: Validation Precision: 0.5083, Validation Recall: 0.8403, Validation F1: 0.6335, Validation Support: 1998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 9/15 => Train Loss: 1.6200, Train Accuracy: 0.5392\n",
      "Angry: Train Precision: 0.4707, Train Recall: 0.4256, Train F1: 0.4470, Train Support: 16061\n",
      "Disgust: Train Precision: 0.6995, Train Recall: 0.7558, Train F1: 0.7266, Train Support: 15967\n",
      "Fear: Train Precision: 0.4014, Train Recall: 0.2084, Train F1: 0.2744, Train Support: 16110\n",
      "Happy: Train Precision: 0.6313, Train Recall: 0.6928, Train F1: 0.6606, Train Support: 15926\n",
      "Neutral: Train Precision: 0.4425, Train Recall: 0.5220, Train F1: 0.4789, Train Support: 15929\n",
      "Sad: Train Precision: 0.3942, Train Recall: 0.4382, Train F1: 0.4151, Train Support: 16080\n",
      "Surprise: Train Precision: 0.6623, Train Recall: 0.7366, Train F1: 0.6975, Train Support: 15927\n",
      "Validation Loss: 1.6002, Validation Accuracy: 0.5608\n",
      "Angry: Validation Precision: 0.5063, Validation Recall: 0.4400, Validation F1: 0.4708, Validation Support: 2007\n",
      "Disgust: Validation Precision: 0.7877, Validation Recall: 0.7566, Validation F1: 0.7719, Validation Support: 2001\n",
      "Fear: Validation Precision: 0.4712, Validation Recall: 0.1748, Validation F1: 0.2550, Validation Support: 1968\n",
      "Happy: Validation Precision: 0.5641, Validation Recall: 0.7717, Validation F1: 0.6518, Validation Support: 1984\n",
      "Neutral: Validation Precision: 0.4454, Validation Recall: 0.5835, Validation F1: 0.5052, Validation Support: 2007\n",
      "Sad: Validation Precision: 0.4369, Validation Recall: 0.4490, Validation F1: 0.4429, Validation Support: 2067\n",
      "Surprise: Validation Precision: 0.6926, Validation Recall: 0.7528, Validation F1: 0.7214, Validation Support: 1966\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                             \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[17], line 91\u001b[0m\n\u001b[0;32m     88\u001b[0m train_loss, train_accuracy, train_metrics_per_class \u001b[38;5;241m=\u001b[39m train_epoch(net, train_loader, criterion, optimizer, device, your_label_mapping)\n\u001b[0;32m     90\u001b[0m \u001b[38;5;66;03m# Validation\u001b[39;00m\n\u001b[1;32m---> 91\u001b[0m val_loss, val_accuracy, val_metrics_per_class \u001b[38;5;241m=\u001b[39m \u001b[43mevaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnet\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mval_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43myour_label_mapping\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m \u001b[38;5;66;03m# Scheduler step based on validation accuracy\u001b[39;00m\n\u001b[0;32m     94\u001b[0m scheduler\u001b[38;5;241m.\u001b[39mstep(val_accuracy)\n",
      "Cell \u001b[1;32mIn[17], line 65\u001b[0m, in \u001b[0;36mevaluate\u001b[1;34m(model, val_loader, criterion, device, label_mapping)\u001b[0m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m inputs, labels \u001b[38;5;129;01min\u001b[39;00m tqdm(val_loader, desc\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValidation\u001b[39m\u001b[38;5;124m'\u001b[39m, leave\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m):\n\u001b[0;32m     63\u001b[0m     inputs, labels \u001b[38;5;241m=\u001b[39m inputs\u001b[38;5;241m.\u001b[39mto(device), labels\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[1;32m---> 65\u001b[0m     outputs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     66\u001b[0m     loss \u001b[38;5;241m=\u001b[39m criterion(outputs, labels)\n\u001b[0;32m     67\u001b[0m     running_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[10], line 48\u001b[0m, in \u001b[0;36mEmotionCNN.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     47\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x):\n\u001b[1;32m---> 48\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmaxpool1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrelu1\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     49\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2a(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2a(x))\n\u001b[0;32m     50\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu2b(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mconv2b(x))\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1518\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1516\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[0;32m   1517\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1518\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\module.py:1527\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1522\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1523\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1524\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1525\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1526\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1527\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1529\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m   1530\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\modules\\pooling.py:166\u001b[0m, in \u001b[0;36mMaxPool2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    165\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor):\n\u001b[1;32m--> 166\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    167\u001b[0m \u001b[43m                        \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mceil_mode\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    168\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mreturn_indices\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreturn_indices\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\_jit_internal.py:488\u001b[0m, in \u001b[0;36mboolean_dispatch.<locals>.fn\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    486\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_true(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m    487\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m--> 488\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m if_false(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n",
      "File \u001b[1;32mc:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\project-venv\\lib\\site-packages\\torch\\nn\\functional.py:791\u001b[0m, in \u001b[0;36m_max_pool2d\u001b[1;34m(input, kernel_size, stride, padding, dilation, ceil_mode, return_indices)\u001b[0m\n\u001b[0;32m    789\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m stride \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m    790\u001b[0m     stride \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mjit\u001b[38;5;241m.\u001b[39mannotate(List[\u001b[38;5;28mint\u001b[39m], [])\n\u001b[1;32m--> 791\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmax_pool2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkernel_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mceil_mode\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# definition of the model, criterion, optimizer and scheduler\n",
    "net = EmotionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "\n",
    "def calculate_metrics_per_class(true_labels, predicted_labels, label_mapping):\n",
    "    unique_labels = list(label_mapping.keys())\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, labels=unique_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    metrics_per_class = {}\n",
    "    for i, idx in enumerate(unique_labels):\n",
    "        metrics_per_class[idx] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "\n",
    "    return accuracy, metrics_per_class\n",
    "\n",
    "# function for training\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, label_mapping):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# function for evaluation\n",
    "def evaluate(model, val_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(val_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# Settings\n",
    "num_epochs = epochs_\n",
    "early_stopping_patience = 3  # numbers of epochs with no improvement after which training will be stopped (early stopping)\n",
    "best_accuracy = 0.0\n",
    "best_epoch = 0\n",
    "no_improvement_count = 0\n",
    "\n",
    "# Training cycle\n",
    "for epoch in range(num_epochs):\n",
    "    # Training\n",
    "    train_loss, train_accuracy, train_metrics_per_class = train_epoch(net, train_loader, criterion, optimizer, device, your_label_mapping)\n",
    "\n",
    "    # Validation\n",
    "    val_loss, val_accuracy, val_metrics_per_class = evaluate(net, val_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "    # Scheduler step based on validation accuracy\n",
    "    scheduler.step(val_accuracy)\n",
    "\n",
    "    # Saving the model if the current accuracy is better than the best\n",
    "    if val_accuracy > best_accuracy:\n",
    "        best_accuracy = val_accuracy\n",
    "        best_epoch = epoch\n",
    "        torch.save(net.state_dict(), r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model.pth')\n",
    "        no_improvement_count = 0\n",
    "    else:\n",
    "        no_improvement_count += 1\n",
    "\n",
    "    # Print epoch statistics\n",
    "    print(f'Epoch {epoch + 1}/{num_epochs} => '\n",
    "          f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "    # Print metrics per class\n",
    "    for idx, label in your_label_mapping.items():\n",
    "        print(f'{label}: Train Precision: {train_metrics_per_class[idx][\"precision\"]:.4f}, Train Recall: {train_metrics_per_class[idx][\"recall\"]:.4f}, Train F1: {train_metrics_per_class[idx][\"f1\"]:.4f}, Train Support: {train_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "    print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "    \n",
    "    for idx, label in your_label_mapping.items():\n",
    "        print(f'{label}: Validation Precision: {val_metrics_per_class[idx][\"precision\"]:.4f}, Validation Recall: {val_metrics_per_class[idx][\"recall\"]:.4f}, Validation F1: {val_metrics_per_class[idx][\"f1\"]:.4f}, Validation Support: {val_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "    if no_improvement_count >= early_stopping_patience:\n",
    "        print(f'Early stopping at epoch {epoch + 1} as there is no improvement in validation accuracy for {early_stopping_patience} consecutive epochs.')\n",
    "        break\n",
    "\n",
    "print(f'Best model achieved at epoch {best_epoch + 1} with accuracy {best_accuracy:.4f}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the final model with the test set of the same dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.3903, Test Accuracy: 0.7750\n",
      "Angry: Test Precision: 0.7497, Test Recall: 0.7361, Test F1: 0.7429, Test Support: 2967\n",
      "Disgust: Test Precision: 0.9443, Test Recall: 0.8830, Test F1: 0.9126, Test Support: 2975\n",
      "Fear: Test Precision: 0.7215, Test Recall: 0.6304, Test F1: 0.6729, Test Support: 2922\n",
      "Happy: Test Precision: 0.7768, Test Recall: 0.8618, Test F1: 0.8171, Test Support: 3082\n",
      "Neutral: Test Precision: 0.7173, Test Recall: 0.7299, Test F1: 0.7235, Test Support: 3017\n",
      "Sad: Test Precision: 0.6742, Test Recall: 0.7450, Test F1: 0.7078, Test Support: 3039\n",
      "Surprise: Test Precision: 0.8606, Test Recall: 0.8339, Test F1: 0.8470, Test Support: 2998\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics_per_class(true_labels, predicted_labels, label_mapping):\n",
    "    unique_labels = list(label_mapping.keys())\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, labels=unique_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    metrics_per_class = {}\n",
    "    for i, idx in enumerate(unique_labels):\n",
    "        metrics_per_class[idx] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "\n",
    "    return accuracy, metrics_per_class\n",
    "\n",
    "# function for testing\n",
    "def test(model, test_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(test_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the model\n",
    "best_model = EmotionCNN()\n",
    "best_model.load_state_dict(torch.load(r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model_paper_1_20_epochs_bs_48_30k.pth', map_location=device))\n",
    "best_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "test_loss, test_accuracy, test_metrics_per_class = test(best_model, test_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "# print the metrics per class\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "for idx, label in your_label_mapping.items():\n",
    "    print(f'{label}: Test Precision: {test_metrics_per_class[idx][\"precision\"]:.4f}, Test Recall: {test_metrics_per_class[idx][\"recall\"]:.4f}, Test F1: {test_metrics_per_class[idx][\"f1\"]:.4f}, Test Support: {test_metrics_per_class[idx][\"support\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Test the final model with a different dataset**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformation definition\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\test_images_emotion'\n",
    "\n",
    "# create an instance of ImageFolder with the transformations\n",
    "dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "# seed = 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# extract the labels and the indices of the dataset\n",
    "labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "# convert the list into a tensor\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# calculate the number of instances for each class\n",
    "counts = torch.bincount(labels)\n",
    "\n",
    "# calculate the weights for each class\n",
    "weights = 1.0 / counts.float()\n",
    "\n",
    "# create a weight vector for each index in the dataset\n",
    "sample_weights = weights[labels]\n",
    "\n",
    "# set the number of samples for the train set and the test set\n",
    "train_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "val_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "test_size = (number_instances_over_under_sampling_/10) * 7 * 0.8\n",
    "\n",
    "# crea un sampler per il train set and one for the test set\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "# create a dataloader for the train set and the test set with the corresponding samplers\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "val_loader = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Testing:   0%|          | 0/290 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                          "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test Loss: 1.7549, Test Accuracy: 0.4045\n",
      "Angry: Test Precision: 0.3897, Test Recall: 0.4119, Test F1: 0.4005, Test Support: 2367\n",
      "Disgust: Test Precision: 0.5617, Test Recall: 0.0959, Test F1: 0.1638, Test Support: 2420\n",
      "Fear: Test Precision: 0.4656, Test Recall: 0.2522, Test F1: 0.3272, Test Support: 2391\n",
      "Happy: Test Precision: 0.3737, Test Recall: 0.9099, Test F1: 0.5299, Test Support: 2497\n",
      "Neutral: Test Precision: 0.3302, Test Recall: 0.4018, Test F1: 0.3625, Test Support: 2275\n",
      "Sad: Test Precision: 0.3552, Test Recall: 0.3682, Test F1: 0.3616, Test Support: 2428\n",
      "Surprise: Test Precision: 0.7382, Test Recall: 0.3737, Test F1: 0.4962, Test Support: 2422\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "def calculate_metrics_per_class(true_labels, predicted_labels, label_mapping):\n",
    "    unique_labels = list(label_mapping.keys())\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, labels=unique_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    metrics_per_class = {}\n",
    "    for i, idx in enumerate(unique_labels):\n",
    "        metrics_per_class[idx] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "\n",
    "    return accuracy, metrics_per_class\n",
    "\n",
    "# function for the test\n",
    "def test(model, test_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(test_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# load the model \n",
    "best_model = EmotionCNN()\n",
    "best_model.load_state_dict(torch.load(r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model_paper_1_20_epochs_bs_48_30k.pth\", map_location=device))\n",
    "best_model.to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "test_loss, test_accuracy, test_metrics_per_class = test(best_model, test_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "# print the loss and the accuracy of the model\n",
    "print(f'Test Loss: {test_loss:.4f}, Test Accuracy: {test_accuracy:.4f}')\n",
    "\n",
    "for idx, label in your_label_mapping.items():\n",
    "    print(f'{label}: Test Precision: {test_metrics_per_class[idx][\"precision\"]:.4f}, Test Recall: {test_metrics_per_class[idx][\"recall\"]:.4f}, Test F1: {test_metrics_per_class[idx][\"f1\"]:.4f}, Test Support: {test_metrics_per_class[idx][\"support\"]}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Live emotion detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "model = EmotionCNN(num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model_paper_1_20_epochs_bs_48_30k.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# initialize the face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# apply the transformations to the face image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # faces detection\n",
    "    faces = detector(frame)\n",
    "\n",
    "    # if there is at least one face detected, process the image\n",
    "    if len(faces) > 0:\n",
    "        # take only the first face\n",
    "        face = faces[0]\n",
    "        \n",
    "        # cut the face from the frame\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # check if the face image is not empty\n",
    "        if not face_image.size == 0:\n",
    "            # apply the transformations to the face image\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "            input_image = transform(pil_image).unsqueeze(0)  # Aggiunge una dimensione di batch\n",
    "            input_image = input_image.to(device)\n",
    "\n",
    "            # model prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_image)\n",
    "\n",
    "            # get the label predicted by the model\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_emotion = your_label_mapping[predicted.item()]\n",
    "\n",
    "            print(f'Predicted Emotion: {predicted_emotion}')\n",
    "\n",
    "    # show the frame with the face rectangle added\n",
    "    cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "    # wait for 2 seconds (time in milliseconds)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    # if q is pressed, terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
