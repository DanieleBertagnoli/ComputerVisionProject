{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WINDOWS (your env must be called project-venv; if you choose another name add it in .gitignore)\n",
    "import subprocess\n",
    "\n",
    "# Set the execution policy\n",
    "subprocess.run([\"Set-ExecutionPolicy\", \"RemoteSigned\", \"-Scope\", \"Process\"], shell=True)\n",
    "\n",
    "# Activate the virtual environment\n",
    "subprocess.run([\"cd\", \".\\\\project-venv\\\\\"], shell=True)\n",
    "subprocess.run([\".\\\\Scripts\\\\Activate.ps1\"], shell=True)\n",
    "\n",
    "# Install requirements\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../requirements.txt\"], shell=True)\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../emotions_requirements.txt\"], shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "########## dlib_face_recognition_resnet_model_v1.dat ################\n",
    "\n",
    "# URL del file di Google Drive\n",
    "url_1 = 'https://drive.google.com/uc?id=1tXD6dha1ZD4fceLWsGlI89t8HeHlkJYC' \n",
    "\n",
    "# Percorso in cui si desidera salvare il file scaricato\n",
    "output_1 = '../Models/dlib_face_recognition_resnet_model_v1.dat'\n",
    "\n",
    "gdown.download(url_1, output_1, quiet=False)\n",
    "\n",
    "\n",
    "\n",
    "########## shape_predictor_68_face_landmarks.dat ###################\n",
    "\n",
    "# URL del file di Google Drive\n",
    "url_2 = 'https://drive.google.com/uc?id=1dvIeJtWhObCgSYJt8WKnjIlHhw5Y9ioN'\n",
    "\n",
    "# Percorso in cui si desidera salvare il file scaricato\n",
    "output_2 = '../Models/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "gdown.download(url_2, output_2, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**import packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import os\n",
    "import random\n",
    "import numpy as np\n",
    "from scipy.ndimage import gaussian_filter, map_coordinates\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from sklearn.cluster import DBSCAN\n",
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data.sampler import SubsetRandomSampler\n",
    "from torch.optim import SGD\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from sklearn.metrics import precision_recall_fscore_support, accuracy_score\n",
    "from tqdm import tqdm\n",
    "import dlib\n",
    "from PIL import Image\n",
    "import wandb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**wandb login**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "wandb.login(key='d29d51017f4231b5149d36ad242526b374c9c60a')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper implementation 1\n",
    "https://ieeexplore.ieee.org/abstract/document/9659697?casa_token=zDD7lwwOig8AAAAA:KcIHhupXAXgiaB_C7A0uNDB7ehrsWNyovQdgDu9LmnwToOGU6akB_gjWTy7JCf4UdKK03Is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Dataset augmenting**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "emotions = ['angry', 'disgust', 'fear', 'happy', 'neutral', 'sad', 'surprise']\n",
    "\n",
    "# cycle through emotions\n",
    "for emotion in emotions:\n",
    "    # path of the folder containing the images\n",
    "    folder_path = fr\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\\{emotion}\"\n",
    "    output_folder_path = fr\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\\{emotion}_augmented\"\n",
    "\n",
    "\n",
    "    if not os.path.exists(output_folder_path):\n",
    "        os.makedirs(output_folder_path)\n",
    "\n",
    "    # list of images in the folder\n",
    "    image_files = [f for f in os.listdir(folder_path) if f.endswith(('.jpg', '.jpeg', '.png'))]\n",
    "\n",
    "    # define transformations inside the apply_transformations function\n",
    "    def apply_transformations(image):\n",
    "        # horizontal_flip\n",
    "        flipped_horizontal = cv2.flip(image, 1)\n",
    "\n",
    "        # vertical flip\n",
    "        flipped_vertical = cv2.flip(image, 0)\n",
    "\n",
    "        # Zoom\n",
    "        zoom_factor = random.uniform(0.8, 1.2)\n",
    "        height, width = image.shape[:2]\n",
    "        zoomed_image = cv2.resize(image, (int(width * zoom_factor), int(height * zoom_factor)))\n",
    "\n",
    "        # translation\n",
    "        tx = random.randint(-10, 10)\n",
    "        ty = random.randint(-10, 10)\n",
    "        translation_matrix = np.float32([[1, 0, tx], [0, 1, ty]])\n",
    "        translated_image = cv2.warpAffine(image, translation_matrix, (width, height))\n",
    "\n",
    "        # contrast and brightness control\n",
    "        alpha = random.uniform(0.8, 1.2)\n",
    "        beta = random.randint(-35, 35)\n",
    "        adjusted_image = cv2.convertScaleAbs(image, alpha=alpha, beta=beta)\n",
    "\n",
    "        # elastic transformation\n",
    "        elastic_image = elastic_transform(image, alpha=random.randint(6, 14), sigma=random.uniform(1.1, 2.0))\n",
    "\n",
    "        return [image, translated_image, flipped_horizontal, zoomed_image, adjusted_image, elastic_image] #forse togliere flipped vertical\n",
    "\n",
    "    def elastic_transform(image, alpha, sigma):\n",
    "        random_state = np.random.RandomState(None)\n",
    "        shape = image.shape\n",
    "        dx = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dy = gaussian_filter((random_state.rand(*shape) * 2 - 1), sigma, mode=\"constant\", cval=0) * alpha\n",
    "        dz = np.zeros_like(dx)\n",
    "\n",
    "        x, y, z = np.meshgrid(np.arange(shape[1]), np.arange(shape[0]), np.arange(shape[2]))\n",
    "        indices = np.reshape(y + dy, (-1, 1)), np.reshape(x + dx, (-1, 1)), np.reshape(z + dz, (-1, 1))\n",
    "\n",
    "        distorted_image = map_coordinates(image, indices, order=1, mode='reflect')\n",
    "        distorted_image = distorted_image.reshape(image.shape)\n",
    "\n",
    "        return distorted_image\n",
    "\n",
    "    # apply data augmentation\n",
    "    for image_file in image_files:\n",
    "        image_path = os.path.join(folder_path, image_file)\n",
    "        image = cv2.imread(image_path)\n",
    "\n",
    "        augmented_images = apply_transformations(image)\n",
    "\n",
    "        # save new images\n",
    "        base_name = os.path.splitext(image_file)[0]\n",
    "        for i, augmented_image in enumerate(augmented_images):\n",
    "            output_file_path = os.path.join(output_folder_path, f\"{base_name}_aug_{i}.jpg\")\n",
    "            cv2.imwrite(output_file_path, augmented_image)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2a = nn.ReLU()\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2b = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3a = nn.ReLU()\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3b = nn.ReLU()\n",
    "        self.avgpool3 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # verify the output size of conv2 and conv3\n",
    "        self.dummy_input = torch.randn(1, 1, 48, 48)\n",
    "        self.dummy_output_size = self._get_conv_output_size(self.dummy_input)\n",
    "        \n",
    "        # update fc1 units based on feature map size\n",
    "        self.fc1 = nn.Linear(self.dummy_output_size, 1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output_size(self, input_tensor):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(input_tensor)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(self.relu_fc1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu_fc2(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_instances_over_under_sampling_ = 30000\n",
    "batch_size_ = 48\n",
    "epochs_ = 20"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete outliers with dbscan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def remove_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # remove outliers\n",
    "    counter = 0\n",
    "    for i, (label, image) in enumerate(zip(labels, os.listdir(folder_path))):\n",
    "        if label == -1:  \n",
    "            image_path = os.path.join(folder_path, image)\n",
    "            os.remove(image_path)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "# DBSCAN configuration\n",
    "dbscan_eps = 0.4  # search radius\n",
    "dbscan_min_samples = 15  # minimum number of samples required for a cluster\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\"\n",
    "\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "    \n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        remove_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        remove_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the number of the outliers per class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def get_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    image_paths = []\n",
    "\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "        image_paths.append(image_path)\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # dbscan to identify outliers\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # collect outlier paths and count for each label\n",
    "    outlier_paths_by_label = {}\n",
    "    for label, image_path in zip(labels, image_paths):\n",
    "        if label == -1:\n",
    "            if label not in outlier_paths_by_label:\n",
    "                outlier_paths_by_label[label] = []\n",
    "            outlier_paths_by_label[label].append(image_path)\n",
    "\n",
    "    return outlier_paths_by_label\n",
    "\n",
    "# Configurazione DBSCAN\n",
    "dbscan_eps = 0.5  # Raggio di ricerca\n",
    "dbscan_min_samples = 10  # Numero minimo di campioni in un cluster\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images\"\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "\n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path \n",
    "        outliers = get_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        outliers = get_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n",
    "\n",
    "    # Stampa i percorsi degli outliers per ogni label\n",
    "    for label, outlier_paths in outliers.items():\n",
    "        print(f\"Label {label}: {len(outlier_paths)} outliers\")\n",
    "        for path in outlier_paths:\n",
    "            print(path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model training with wandb**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# definition of the model, criterion, optimizer and scheduler\n",
    "net = EmotionCNN().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = SGD(net.parameters(), lr=0.01, momentum=0.9, nesterov=True, weight_decay=0.0001)\n",
    "scheduler = ReduceLROnPlateau(optimizer, mode='max', factor=0.75, patience=5, verbose=True)\n",
    "\n",
    "def calculate_metrics_per_class(true_labels, predicted_labels, label_mapping):\n",
    "    unique_labels = list(label_mapping.keys())\n",
    "    precision, recall, f1, support = precision_recall_fscore_support(true_labels, predicted_labels, labels=unique_labels)\n",
    "    accuracy = accuracy_score(true_labels, predicted_labels)\n",
    "    \n",
    "    metrics_per_class = {}\n",
    "    for i, idx in enumerate(unique_labels):\n",
    "        metrics_per_class[idx] = {\n",
    "            'precision': precision[i],\n",
    "            'recall': recall[i],\n",
    "            'f1': f1[i],\n",
    "            'support': support[i]\n",
    "        }\n",
    "\n",
    "    return accuracy, metrics_per_class\n",
    "\n",
    "# function for training\n",
    "def train_epoch(model, train_loader, criterion, optimizer, device, label_mapping):\n",
    "    model.train()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    for inputs, labels in tqdm(train_loader, desc='Training', leave=False):\n",
    "        inputs, labels = inputs.to(device), labels.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        true_labels.extend(labels.cpu().numpy())\n",
    "        predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(train_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# function for evaluation\n",
    "def evaluate(model, val_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(val_loader, desc='Validation', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(val_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "# Funzione per il test\n",
    "def test(model, test_loader, criterion, device, label_mapping):\n",
    "    model.eval()\n",
    "    running_loss = 0.0\n",
    "    true_labels = []\n",
    "    predicted_labels = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, labels in tqdm(test_loader, desc='Testing', leave=False):\n",
    "            inputs, labels = inputs.to(device), labels.to(device)\n",
    "\n",
    "            outputs = model(inputs)\n",
    "            loss = criterion(outputs, labels)\n",
    "            running_loss += loss.item()\n",
    "\n",
    "            _, preds = torch.max(outputs, 1)\n",
    "            true_labels.extend(labels.cpu().numpy())\n",
    "            predicted_labels.extend(preds.cpu().numpy())\n",
    "\n",
    "    average_loss = running_loss / len(test_loader)\n",
    "    accuracy, metrics_per_class = calculate_metrics_per_class(true_labels, predicted_labels, label_mapping)\n",
    "\n",
    "    return average_loss, accuracy, metrics_per_class\n",
    "\n",
    "\n",
    "def train_and_evaluate(config=None):\n",
    "    if config is None:\n",
    "        config = {}\n",
    "    wandb.init(config=config, project=\"nome-progetto\")\n",
    "\n",
    "        # transformation definition\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\test_images_emotion'\n",
    "\n",
    "    # create an instance of ImageFolder with the transformations\n",
    "    dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "    # seed = 42\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # extract the labels and the indices of the dataset\n",
    "    labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "    # convert the list into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # calculate the number of instances for each class\n",
    "    counts = torch.bincount(labels)\n",
    "\n",
    "    # calculate the weights for each class\n",
    "    weights = 1.0 / counts.float()\n",
    "\n",
    "    # create a weight vector for each index in the dataset\n",
    "    sample_weights = weights[labels]\n",
    "\n",
    "    # set the number of samples for the train set and the test set\n",
    "    train_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "    val_size = (number_instances_over_under_sampling_/10) * 7 * 0.1\n",
    "    test_size = (number_instances_over_under_sampling_/10) * 7 * 0.8\n",
    "\n",
    "    # crea un sampler per il train set and one for the test set\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "    val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "    test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "    # create a dataloader for the train set and the test set with the corresponding samplers\n",
    "    train_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "    val_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "    test_loader_ = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "            # transformation definition\n",
    "    transform = transforms.Compose([\n",
    "        transforms.Grayscale(num_output_channels=1),\n",
    "        transforms.Resize((48, 48)),\n",
    "        transforms.ToTensor(),\n",
    "        transforms.Normalize((0.5,), (0.5,))\n",
    "    ])\n",
    "\n",
    "    dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_aug_images'\n",
    "\n",
    "    # create an instance of ImageFolder with the transformations\n",
    "    dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "    # seed = 42\n",
    "    torch.manual_seed(42)\n",
    "\n",
    "    # extract the labels and the indices of the dataset\n",
    "    labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "    # convert the list into a tensor\n",
    "    labels = torch.tensor(labels)\n",
    "\n",
    "    # calculate the number of instances for each class\n",
    "    counts = torch.bincount(labels)\n",
    "\n",
    "    # calculate the weights for each class\n",
    "    weights = 1.0 / counts.float()\n",
    "\n",
    "    # create a weight vector for each index in the dataset\n",
    "    sample_weights = weights[labels]\n",
    "\n",
    "    # set the number of samples for the train set and the test set\n",
    "    train_size = number_instances_over_under_sampling_ * 7 * 0.8\n",
    "    val_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "    test_size = number_instances_over_under_sampling_ * 7 * 0.1\n",
    "\n",
    "    # crea un sampler per il train set and one for the test set\n",
    "    train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "    val_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(val_size))\n",
    "    test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "    # create a dataloader for the train set and the test set with the corresponding samplers\n",
    "    train_loader = DataLoader(dataset, batch_size=batch_size_, sampler=train_sampler, num_workers=4)\n",
    "    val_loader = DataLoader(dataset, batch_size=batch_size_, sampler=val_sampler, num_workers=4)\n",
    "    test_loader = DataLoader(dataset, batch_size=batch_size_, sampler=test_sampler, num_workers=4)\n",
    "\n",
    "    your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "    # Settings\n",
    "    num_epochs = epochs_\n",
    "    early_stopping_patience = 3  # numbers of epochs with no improvement after which training will be stopped (early stopping)\n",
    "    best_accuracy = 0.0\n",
    "    best_epoch = 0\n",
    "    no_improvement_count = 0\n",
    "\n",
    "    train_loss = 0.0\n",
    "    train_accuracy = 0.0\n",
    "    train_metrics_per_class = {}\n",
    "\n",
    "    val_loss = 0.0\n",
    "    val_accuracy = 0.0\n",
    "    val_metrics_per_class = {}\n",
    "\n",
    "    # Training cycle\n",
    "    for epoch in range(num_epochs):\n",
    "        # Training\n",
    "        train_loss, train_accuracy, train_metrics_per_class = train_epoch(net, train_loader, criterion, optimizer, device, your_label_mapping)\n",
    "\n",
    "        # Validation\n",
    "        val_loss, val_accuracy, val_metrics_per_class = evaluate(net, val_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "        # Scheduler step based on validation accuracy\n",
    "        scheduler.step(val_accuracy)\n",
    "\n",
    "        # Saving the model if the current accuracy is better than the best\n",
    "        if val_accuracy > best_accuracy:\n",
    "            best_accuracy = val_accuracy\n",
    "            best_epoch = epoch\n",
    "            torch.save(net.state_dict(), r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model.pth')\n",
    "            no_improvement_count = 0\n",
    "        else:\n",
    "            no_improvement_count += 1\n",
    "\n",
    "        # Print epoch statistics\n",
    "        print(f'Epoch {epoch + 1}/{num_epochs} => '\n",
    "            f'Train Loss: {train_loss:.4f}, Train Accuracy: {train_accuracy:.4f}')\n",
    "\n",
    "        # Print metrics per class\n",
    "        for idx, label in your_label_mapping.items():\n",
    "            print(f'{label}: Train Precision: {train_metrics_per_class[idx][\"precision\"]:.4f}, Train Recall: {train_metrics_per_class[idx][\"recall\"]:.4f}, Train F1: {train_metrics_per_class[idx][\"f1\"]:.4f}, Train Support: {train_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "        print(f'Validation Loss: {val_loss:.4f}, Validation Accuracy: {val_accuracy:.4f}')\n",
    "        \n",
    "        for idx, label in your_label_mapping.items():\n",
    "            print(f'{label}: Validation Precision: {val_metrics_per_class[idx][\"precision\"]:.4f}, Validation Recall: {val_metrics_per_class[idx][\"recall\"]:.4f}, Validation F1: {val_metrics_per_class[idx][\"f1\"]:.4f}, Validation Support: {val_metrics_per_class[idx][\"support\"]}')\n",
    "\n",
    "        if no_improvement_count >= early_stopping_patience:\n",
    "            print(f'Early stopping at epoch {epoch + 1} as there is no improvement in validation accuracy for {early_stopping_patience} consecutive epochs.')\n",
    "            break\n",
    "\n",
    "    print(f'Best model achieved at epoch {best_epoch + 1} with accuracy {best_accuracy:.4f}')\n",
    "\n",
    "    best_model = EmotionCNN()\n",
    "    best_model.load_state_dict(torch.load(r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model_paper_1_20_epochs.pth'))\n",
    "    best_model.to(device)\n",
    "    test_loss, test_accuracy, test_metrics_per_class = test(best_model, test_loader, criterion, device, your_label_mapping)\n",
    "\n",
    "    test_loss_, test_accuracy_, test_metrics_per_class_ = test(best_model, test_loader_, criterion, device, your_label_mapping)\n",
    "\n",
    "    # log the best model\n",
    "    wandb.log({\"train_loss\": train_loss, \"train_accuracy\": train_accuracy, \"val_loss\": val_loss, \"val_accuracy\": val_accuracy, \"test_accuracy\": test_accuracy, \"test_accuracy_\": test_accuracy_})\n",
    "\n",
    "    # log metrics per class\n",
    "    for idx, label in your_label_mapping.items():\n",
    "        wandb.log({\n",
    "            f'train_precision_{label}': train_metrics_per_class[idx][\"precision\"],\n",
    "            f'train_recall_{label}': train_metrics_per_class[idx][\"recall\"],\n",
    "            f'train_f1_{label}': train_metrics_per_class[idx][\"f1\"],\n",
    "            f'train_support_{label}': train_metrics_per_class[idx][\"support\"],\n",
    "        })\n",
    "\n",
    "    for idx, label in your_label_mapping.items():\n",
    "        wandb.log({\n",
    "            f'val_precision_{label}': val_metrics_per_class[idx][\"precision\"],\n",
    "            f'val_recall_{label}': val_metrics_per_class[idx][\"recall\"],\n",
    "            f'val_f1_{label}': val_metrics_per_class[idx][\"f1\"],\n",
    "            f'val_support_{label}': val_metrics_per_class[idx][\"support\"],\n",
    "        })\n",
    "\n",
    "\n",
    "# sweep configuration\n",
    "sweep_config = {\n",
    "    \"method\": \"grid\",\n",
    "    \"parameters\": {\n",
    "        \"number_instances_over_under_sampling_\": {\"values\": [20000, 30000]},\n",
    "        \"batch_size_\": {\"values\": [32, 48, 64]},\n",
    "        \"epochs_\": {\"values\": [10, 15, 20, 25, 30]},\n",
    "    }\n",
    "}\n",
    "\n",
    "# sweep inizialization\n",
    "sweep_id = wandb.sweep(sweep=sweep_config, project=\"nome-progetto\")\n",
    "\n",
    "# sweep execution\n",
    "wandb.agent(sweep_id, function=train_and_evaluate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live emotion detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 7\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "model = EmotionCNN(num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\best_model_paper_1_20_epochs_bs_48_30k.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# initialize the face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# apply the transformations to the face image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # faces detection\n",
    "    faces = detector(frame)\n",
    "\n",
    "    # if there is at least one face detected, process the image\n",
    "    if len(faces) > 0:\n",
    "        # take only the first face\n",
    "        face = faces[0]\n",
    "        \n",
    "        # cut the face from the frame\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # check if the face image is not empty\n",
    "        if not face_image.size == 0:\n",
    "            # apply the transformations to the face image\n",
    "            pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "            input_image = transform(pil_image).unsqueeze(0)  # Aggiunge una dimensione di batch\n",
    "            input_image = input_image.to(device)\n",
    "\n",
    "            # model prediction\n",
    "            with torch.no_grad():\n",
    "                output = model(input_image)\n",
    "\n",
    "            # get the label predicted by the model\n",
    "            _, predicted = torch.max(output, 1)\n",
    "            predicted_emotion = your_label_mapping[predicted.item()]\n",
    "\n",
    "            print(f'Predicted Emotion: {predicted_emotion}')\n",
    "\n",
    "    # show the frame with the face rectangle added\n",
    "    cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "    # wait for 2 seconds (time in milliseconds)\n",
    "    cv2.waitKey(1000)\n",
    "\n",
    "    # if q is pressed, terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
