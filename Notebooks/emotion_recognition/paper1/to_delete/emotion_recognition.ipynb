{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# FOR WINDOWS (your env must be called project-venv; if you choose another name add it in .gitignore)\n",
    "import subprocess\n",
    "\n",
    "# Set the execution policy\n",
    "subprocess.run([\"Set-ExecutionPolicy\", \"RemoteSigned\", \"-Scope\", \"Process\"], shell=True)\n",
    "\n",
    "# Activate the virtual environment\n",
    "subprocess.run([\"cd\", \".\\\\project-venv\\\\\"], shell=True)\n",
    "subprocess.run([\".\\\\Scripts\\\\Activate.ps1\"], shell=True)\n",
    "\n",
    "# Install requirements\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../requirements.txt\"], shell=True)\n",
    "subprocess.run([\"pip\", \"install\", \"-r\", \"../emotions_requirements.txt\"], shell=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "\n",
    "\n",
    "########## dlib_face_recognition_resnet_model_v1.dat ################\n",
    "\n",
    "# URL del file di Google Drive\n",
    "url_1 = 'https://drive.google.com/uc?id=1tXD6dha1ZD4fceLWsGlI89t8HeHlkJYC' \n",
    "\n",
    "# Percorso in cui si desidera salvare il file scaricato\n",
    "output_1 = '../Models/dlib_face_recognition_resnet_model_v1.dat'\n",
    "\n",
    "gdown.download(url_1, output_1, quiet=False)\n",
    "\n",
    "\n",
    "\n",
    "########## shape_predictor_68_face_landmarks.dat ###################\n",
    "\n",
    "# URL del file di Google Drive\n",
    "url_2 = 'https://drive.google.com/uc?id=1dvIeJtWhObCgSYJt8WKnjIlHhw5Y9ioN'\n",
    "\n",
    "# Percorso in cui si desidera salvare il file scaricato\n",
    "output_2 = '../Models/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "gdown.download(url_2, output_2, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Emotion Recognition task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Paper implementation 1\n",
    "https://ieeexplore.ieee.org/abstract/document/9659697?casa_token=zDD7lwwOig8AAAAA:KcIHhupXAXgiaB_C7A0uNDB7ehrsWNyovQdgDu9LmnwToOGU6akB_gjWTy7JCf4UdKK03Is"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CNN architecture**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class EmotionCNN(nn.Module):\n",
    "    def __init__(self, num_classes=7):\n",
    "        super(EmotionCNN, self).__init__()\n",
    "        \n",
    "        self.conv1 = nn.Conv2d(1, 64, kernel_size=5, stride=1, padding=0)\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.maxpool1 = nn.MaxPool2d(kernel_size=5, stride=2)\n",
    "        \n",
    "        self.conv2a = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2a = nn.ReLU()\n",
    "        self.conv2b = nn.Conv2d(64, 64, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu2b = nn.ReLU()\n",
    "        self.avgpool2 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        self.conv3a = nn.Conv2d(64, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3a = nn.ReLU()\n",
    "        self.conv3b = nn.Conv2d(128, 128, kernel_size=3, stride=1, padding=1)\n",
    "        self.relu3b = nn.ReLU()\n",
    "        self.avgpool3 = nn.AvgPool2d(kernel_size=3, stride=2)\n",
    "        \n",
    "        # verify the output size of conv2 and conv3\n",
    "        self.dummy_input = torch.randn(1, 1, 48, 48)\n",
    "        self.dummy_output_size = self._get_conv_output_size(self.dummy_input)\n",
    "        \n",
    "        # update fc1 units based on feature map size\n",
    "        self.fc1 = nn.Linear(self.dummy_output_size, 1024)\n",
    "        self.relu_fc1 = nn.ReLU()\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc2 = nn.Linear(1024, 1024)\n",
    "        self.relu_fc2 = nn.ReLU()\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        \n",
    "        self.fc3 = nn.Linear(1024, num_classes)\n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "\n",
    "    def _get_conv_output_size(self, input_tensor):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(input_tensor)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        return x.view(x.size(0), -1).size(1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.maxpool1(self.relu1(self.conv1(x)))\n",
    "        x = self.relu2a(self.conv2a(x))\n",
    "        x = self.relu2b(self.conv2b(x))\n",
    "        x = self.avgpool2(x)\n",
    "        x = self.relu3a(self.conv3a(x))\n",
    "        x = self.relu3b(self.conv3b(x))\n",
    "        x = self.avgpool3(x)\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.dropout1(self.relu_fc1(self.fc1(x)))\n",
    "        x = self.dropout2(self.relu_fc2(self.fc2(x)))\n",
    "        x = self.softmax(self.fc3(x))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**hyperparameters**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "delete_outliers_k_means = False\n",
    "delete_outliers_dbscan = False\n",
    "no_delete_outliers = False\n",
    "\n",
    "number_instances_over_under_sampling = 5000 # fatto\n",
    "over_under_sampling = False\n",
    "no_over_under_sampling = False\n",
    "\n",
    "batch_size_ = 32\n",
    "epochs_ = 30"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete outliers with k-means**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def remove_outliers_in_folder(folder_path, threshold):\n",
    "    counter = 0\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        \n",
    "        if pixel_std > threshold:\n",
    "            counter += 1\n",
    "            os.remove(image_path)\n",
    "    print(counter)\n",
    "\n",
    "# method based on k-means clustering\n",
    "def calculate_threshold_kmeans(values, k=1):\n",
    "    from sklearn.cluster import KMeans\n",
    "    kmeans = KMeans(n_clusters=k, random_state=0).fit(values.reshape(-1, 1))\n",
    "    centroids = kmeans.cluster_centers_.flatten()\n",
    "    threshold = np.mean(centroids)\n",
    "    return threshold\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\"\n",
    "pixel_stds = []\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    for filename in os.listdir(emotion_folder_path):\n",
    "        image_path = os.path.join(emotion_folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        pixel_stds.append(pixel_std)\n",
    "\n",
    "pixel_stds = np.array(pixel_stds)\n",
    "\n",
    "outlier_threshold = calculate_threshold_kmeans(pixel_stds)\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    print(emotion)\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    remove_outliers_in_folder(emotion_folder_path, outlier_threshold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**delete outliers with dbscan**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "391\n",
      "80\n",
      "300\n",
      "239\n",
      "325\n",
      "334\n",
      "289\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "def calculate_pixel_std(image_path):\n",
    "    image = cv2.imread(image_path, cv2.IMREAD_GRAYSCALE)\n",
    "    return np.std(image)\n",
    "\n",
    "def remove_outliers_dbscan(folder_path, eps, min_samples):\n",
    "    images = []\n",
    "    for filename in os.listdir(folder_path):\n",
    "        image_path = os.path.join(folder_path, filename)\n",
    "        pixel_std = calculate_pixel_std(image_path)\n",
    "        images.append([pixel_std])\n",
    "\n",
    "    images = np.array(images)\n",
    "\n",
    "    # Utilizza DBSCAN per identificare gli outlier\n",
    "    dbscan = DBSCAN(eps=eps, min_samples=min_samples)\n",
    "    labels = dbscan.fit_predict(images)\n",
    "\n",
    "    # Rimuovi gli outlier\n",
    "    counter = 0\n",
    "    for i, (label, image) in enumerate(zip(labels, os.listdir(folder_path))): # usa zip e una variabile diversa per l'immagine\n",
    "        if label == -1:  # -1 rappresenta gli outlier\n",
    "            image_path = os.path.join(folder_path, image) # usa la variabile image invece di i\n",
    "            os.remove(image_path)\n",
    "            counter += 1\n",
    "    print(counter)\n",
    "\n",
    "# Configurazione DBSCAN\n",
    "dbscan_eps = 0.2  # Raggio di ricerca\n",
    "dbscan_min_samples = 15  # Numero minimo di campioni in un cluster\n",
    "\n",
    "emotions_folder_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\"\n",
    "\n",
    "\n",
    "for emotion in os.listdir(emotions_folder_path):\n",
    "    emotion_folder_path = os.path.join(emotions_folder_path, emotion)\n",
    "    print(emotion)\n",
    "    \n",
    "    if emotion == 'disgust':\n",
    "        tmp_folder_path = emotion_folder_path  # Usa lo stesso per la classe \"disgust\"\n",
    "        remove_outliers_dbscan(tmp_folder_path, 0.5, 10)\n",
    "    else:\n",
    "        remove_outliers_dbscan(emotion_folder_path, dbscan_eps, dbscan_min_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**with over and under sampling**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1-** Using this method, a random selection of number_instances_over_under_sampling instances is made for each class. ########################################################################"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split, ConcatDataset\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "import numpy as np\n",
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "# transformation definition\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images'\n",
    "\n",
    "# create an instance of ImageFolder with the transformations\n",
    "dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "# seed = 42\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# extract the labels and the indices of the dataset\n",
    "labels = [label for _, label in dataset.imgs]\n",
    "\n",
    "# convert the list into a tensor\n",
    "labels = torch.tensor(labels)\n",
    "\n",
    "# calculate the number of instances for each class\n",
    "counts = torch.bincount(labels)\n",
    "\n",
    "# calculate the weights for each class\n",
    "weights = 1.0 / counts.float()\n",
    "\n",
    "# create a weight vector for each index in the dataset\n",
    "sample_weights = weights[labels]\n",
    "\n",
    "# set the number of samples for the train set and the test set\n",
    "train_size = number_instances_over_under_sampling * 7 * 0.8 \n",
    "test_size = number_instances_over_under_sampling * 7 * 0.2 \n",
    "\n",
    "# crea un sampler per il train set and one for the test set\n",
    "train_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(train_size))\n",
    "test_sampler = torch.utils.data.WeightedRandomSampler(sample_weights, int(test_size))\n",
    "\n",
    "# create a dataloader for the train set and the test set with the corresponding samplers\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(dataset, batch_size=batch_size, sampler=train_sampler, num_workers=4)\n",
    "test_loader = DataLoader(dataset, batch_size=batch_size, sampler=test_sampler, num_workers=4)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifying if for each label there are number_instances_over_under_sampling instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of instances for class in the train set: tensor([1593, 1582, 1606, 1537, 1622, 1635, 1625])\n",
      "number of instances for class in the test set: tensor([398, 386, 391, 408, 398, 411, 408])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_10128\\1737902667.py:4: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  train_counts = torch.bincount(torch.tensor(labels)[train_indices])\n",
      "C:\\Users\\marco\\AppData\\Local\\Temp\\ipykernel_10128\\1737902667.py:8: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  test_counts = torch.bincount(torch.tensor(labels)[test_indices])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "train_indices = list(train_loader.sampler)\n",
    "train_counts = torch.bincount(torch.tensor(labels)[train_indices])\n",
    "print(\"number of instances for class in the train set:\", train_counts)\n",
    "\n",
    "test_indices = list(test_loader.sampler)\n",
    "test_counts = torch.bincount(torch.tensor(labels)[test_indices])\n",
    "print(\"number of instances for class in the test set:\", test_counts)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2-** Questo metodo elimina istanze se superano 5000 e le crea usando smote se sono meno di 5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "verifying if for each label there are number_instances_over_under_sampling instances"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(730, 663)"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**without over and under sampling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import random_split\n",
    "from torchvision.datasets import ImageFolder\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "dataset_root = r'C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images'\n",
    "\n",
    "\n",
    "dataset = ImageFolder(root=dataset_root, transform=transform)\n",
    "\n",
    "torch.manual_seed(42)\n",
    "\n",
    "labels = [label for _, label in dataset.imgs]\n",
    "indices = list(range(len(dataset)))\n",
    "\n",
    "train_indices, test_indices = train_test_split(indices, test_size=0.2, shuffle=True, random_state=42)\n",
    "\n",
    "train_dataset = torch.utils.data.Subset(dataset, train_indices)\n",
    "test_dataset = torch.utils.data.Subset(dataset, test_indices)\n",
    "\n",
    "batch_size = 32\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=4)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False, num_workers=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(350, 88)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_loader), len(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "At the beginning these are the number of instances for each class"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Class angry: 4953 istances\n",
      "Class disgust: 547 istances\n",
      "Class fear: 5121 istances\n",
      "Class happy: 8989 istances\n",
      "Class neutral: 6198 istances\n",
      "Class sad: 6077 istances\n",
      "Class surprise: 4002 istances\n"
     ]
    }
   ],
   "source": [
    "# Ottieni le classi (etichette)\n",
    "classes = dataset.classes\n",
    "\n",
    "# Conta le istanze per ogni classe\n",
    "instances_per_class = {cls: 0 for cls in classes}\n",
    "\n",
    "for _, label in dataset.imgs:\n",
    "    instances_per_class[classes[label]] += 1\n",
    "\n",
    "# Stampa il numero di istanze per ogni classe\n",
    "for cls, count in instances_per_class.items():\n",
    "    print(f\"Class {cls}: {count} istances\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**model training**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import precision_score, recall_score, accuracy_score\n",
    "\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "\n",
    "num_classes = 7\n",
    "model = EmotionCNN(num_classes)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "num_epochs = 30\n",
    "for epoch in range(num_epochs):\n",
    "    for images, labels in train_loader:\n",
    "\n",
    "        outputs = model(images)\n",
    "        loss = criterion(outputs, labels)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        total_correct = 0\n",
    "        total_samples = 0\n",
    "        all_predicted = []\n",
    "        all_labels = []\n",
    "        \n",
    "        for images, labels in test_loader:\n",
    "            outputs = model(images)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            all_predicted.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(labels.cpu().numpy())\n",
    "            total_samples += labels.size(0)\n",
    "            total_correct += (predicted == labels).sum().item()\n",
    "\n",
    "        accuracy = accuracy_score(all_labels, all_predicted)\n",
    "        print(f'Epoch [{epoch + 1}/{num_epochs}], Loss: {loss.item()}, Test Accuracy: {accuracy}')\n",
    "\n",
    "        precision = precision_score(all_labels, all_predicted, average=None)\n",
    "        recall = recall_score(all_labels, all_predicted, average=None)\n",
    "\n",
    "        for i, emotion in enumerate(your_label_mapping.values()):\n",
    "            print(f'Class: {emotion}, Precision: {precision[i]}, Recall: {recall[i]}')\n",
    "\n",
    "        print(f'Total Accuracy: {accuracy}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "**save and use the model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\model2.pth\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**image emotion detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Emotion: Happy\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\model1.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval()\n",
    "\n",
    "from PIL import Image\n",
    "\n",
    "\n",
    "image_path = r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Data\\emotions_images\\happy\\Training_99183228.jpg\"  # Sostituisci con il percorso effettivo dell'immagine\n",
    "image = Image.open(image_path)\n",
    "\n",
    "\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "\n",
    "input_image = transform(image).unsqueeze(0)\n",
    "input_image = input_image.to(device)\n",
    "\n",
    "\n",
    "with torch.no_grad():\n",
    "    output = model(input_image)\n",
    "\n",
    "_, predicted = torch.max(output, 1)\n",
    "predicted_emotion = your_label_mapping[predicted.item()]\n",
    "\n",
    "print(f'Predicted Emotion: {predicted_emotion}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Live emotion detection**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import dlib\n",
    "import torch\n",
    "from torchvision import transforms\n",
    "from PIL import Image\n",
    "\n",
    "num_classes = 7\n",
    "your_label_mapping = {0: 'Angry', 1: 'Disgust', 2: 'Fear', 3: 'Happy', 4: 'Neutral', 5: 'Sad', 6: 'Surprise'}\n",
    "model = EmotionCNN(num_classes)\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "model.load_state_dict(torch.load(r\"C:\\Users\\marco\\OneDrive\\Documenti\\CV_project\\ComputerVisionProject\\Models\\paper1_models\\model2.pth\", map_location=device))\n",
    "model.to(device)\n",
    "model.eval() \n",
    "\n",
    "# initialize the face detector\n",
    "detector = dlib.get_frontal_face_detector()\n",
    "\n",
    "# initialize the camera\n",
    "cap = cv2.VideoCapture(0)\n",
    "\n",
    "# apply the transformations to the face image\n",
    "transform = transforms.Compose([\n",
    "    transforms.Grayscale(num_output_channels=1),\n",
    "    transforms.Resize((48, 48)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize((0.5,), (0.5,))\n",
    "])\n",
    "\n",
    "while True:\n",
    "    # read a frame from the camera\n",
    "    ret, frame = cap.read()\n",
    "\n",
    "    # faces detection\n",
    "    faces = detector(frame)\n",
    "\n",
    "    # if there is at least one face detected, process the image\n",
    "    if len(faces) > 0:\n",
    "        # take only the first face\n",
    "        face = faces[0]\n",
    "        \n",
    "        # cut the face from the frame\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        face_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        # apply the transformations to the face image\n",
    "        pil_image = Image.fromarray(cv2.cvtColor(face_image, cv2.COLOR_BGR2RGB))\n",
    "        input_image = transform(pil_image).unsqueeze(0)  # Aggiunge una dimensione di batch\n",
    "        input_image = input_image.to(device)\n",
    "\n",
    "        # model prediction\n",
    "        with torch.no_grad():\n",
    "            output = model(input_image)\n",
    "\n",
    "        # get the label predicted by the model\n",
    "        _, predicted = torch.max(output, 1)\n",
    "        predicted_emotion = your_label_mapping[predicted.item()]\n",
    "\n",
    "        print(f'Predicted Emotion: {predicted_emotion}')\n",
    "\n",
    "    # show the frame with the face rectangle added\n",
    "    cv2.imshow(\"Face Detection\", frame)\n",
    "\n",
    "    # wait for 2 seconds (time in milliseconds)\n",
    "    cv2.waitKey(2000)\n",
    "\n",
    "    # if q is pressed, terminate the loop\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'):\n",
    "        break\n",
    "\n",
    "# release the capture\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
