{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installing Requirements**\n",
    "\n",
    "Since the project is developed by different people, we will install all the requirements using the requirements.txt file which specifies all the packets' version that must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Downloading Files from GDrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gdown\n",
    "import os \n",
    "\n",
    "if not os.path.exists(\"../Models\"):\n",
    "    os.mkdir(\"../Models\")\n",
    "\n",
    "########## dlib_face_recognition_resnet_model_v1.dat ################\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1tXD6dha1ZD4fceLWsGlI89t8HeHlkJYC' \n",
    "output = '../Models/dlib_face_recognition_resnet_model_v1.dat'\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "\n",
    "########## shape_predictor_68_face_landmarks.dat ###################\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1dvIeJtWhObCgSYJt8WKnjIlHhw5Y9ioN'\n",
    "output = '../Models/shape_predictor_68_face_landmarks.dat'\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "\n",
    "########## 6DRepNet_300W_LP_AFLW2000.pth ###################\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1YY4THdzFA6if-hDLYt6SNytCmlRtw2uo'\n",
    "output = '../Models/6DRepNet_300W_LP_AFLW2000.pth'\n",
    "\n",
    "gdown.download(url, output, quiet=False)\n",
    "\n",
    "\n",
    "########## eth-xgaze_resnet18.pth ###################\n",
    "\n",
    "url = 'https://drive.google.com/uc?id=1gS5PVRGRnecrSa9iIEF5catqufk-j-IT'\n",
    "output = '../Models/eth-xgaze_resnet18.pth'\n",
    "\n",
    "gdown.download(url, output, quiet=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Game Management**\n",
    "\n",
    "In this notebook, we will implement the game management system, including communication with the game and all associated mechanisms. The Python code handles the computer vision aspect of the project, implementing:\n",
    "\n",
    "- Face Registration and Recognition, used for users' signup and login.\n",
    "- Emotion Recognition, to adjust the difficulty of the game based on the player's emotions.\n",
    "- Body Tracking, to control the virtual character."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Face Registration (User Signup)**\n",
    "\n",
    "This section involves recording a 10-second video from the webcam. The frames composing the video will be processed to identify a feasible one in which a face is detected. This process utilizes dlib's face detector, known for its high precision and accuracy. The frames deemed valid will be stored in the UserFaces directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Video Acquisition\n",
    "\n",
    "This Python script captures video from a webcam, saves it to a file, and performs face detection using OpenCV and dlib. The threaded design sets a stop flag after 10 seconds to gracefully terminate the video capture loop. The recorded video frames are stored in a directory named after the provided username. Face detection is executed on the captured video, and the temporary file is removed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import threading\n",
    "import time\n",
    "import os\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "# Thread-safe variable\n",
    "stop_thread = False\n",
    "lock = threading.Lock()\n",
    "\n",
    "def set_stop_thread():\n",
    "\n",
    "    \"\"\"\n",
    "    Thread function to set the stop_thread variable to True after 10 seconds.\n",
    "    \"\"\"\n",
    "    \n",
    "    global stop_thread\n",
    "    time.sleep(10)  # Wait for 10 seconds\n",
    "    with lock:\n",
    "        stop_thread = True\n",
    "\n",
    "def video_acquisition(username: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Capture video from the webcam, save it to a file, and perform face detection.\n",
    "\n",
    "    Parameters:\n",
    "    - username: User identifier for creating output folder\n",
    "\n",
    "    Returns:\n",
    "    - True if faces are detected, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    # Start the thread that sets the stop_thread variable after 10 seconds\n",
    "    stop_thread_thread = threading.Thread(target=set_stop_thread)\n",
    "    stop_thread_thread.start()\n",
    "\n",
    "    # Ensure the directory exists for storing face images\n",
    "    output_directory = f'../UserFaces/{username}/'\n",
    "    os.makedirs(output_directory, exist_ok=True)\n",
    "    video_path = os.path.join(output_directory, 'video.avi') # Define the path for storing the captured video\n",
    "\n",
    "    capture = cv2.VideoCapture(0) # Open a connection to the webcam\n",
    "\n",
    "    # Define the codec and create a VideoWriter object to save the video\n",
    "    fourcc = cv2.VideoWriter_fourcc('X', 'V', 'I', 'D')\n",
    "    videoWriter = cv2.VideoWriter(video_path, fourcc, 30.0, (640, 480), True)\n",
    "\n",
    "    while True:\n",
    "        ret, frame = capture.read() # Read a frame from the webcam\n",
    "\n",
    "        if ret:\n",
    "            cv2.imshow('Webcam', frame)  # Display the frame from the webcam\n",
    "            videoWriter.write(frame) # Write the frame to the video file\n",
    "\n",
    "        with lock:\n",
    "            # Check if the stop_thread variable is set to True\n",
    "            if stop_thread:\n",
    "                break\n",
    "        \n",
    "        if cv2.waitKey(1) == 27:\n",
    "            break\n",
    "\n",
    "    # Release the capture and videoWriter objects\n",
    "    capture.release()\n",
    "    videoWriter.release()\n",
    "\n",
    "    cv2.destroyAllWindows() # Close the OpenCV window\n",
    "    stop_thread_thread.join() # Wait for the stop_thread thread to finish"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Frame Proccesing\n",
    "\n",
    "The process_frame function detects faces in a single input frame using the dlib library, returning True if faces are found and False otherwise. The detect_faces function processes a video, saves frames with detected faces, and returns True if a sufficient number of faces are detected across frames, otherwise False. Using these functions we can simply create a 'database' for each user."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_frame(frame):\n",
    "\n",
    "    \"\"\"\n",
    "    Process a single frame to detect faces.\n",
    "\n",
    "    Parameters:\n",
    "    - frame: Input frame (image)\n",
    "\n",
    "    Returns:\n",
    "    - True if faces are detected, False otherwise\n",
    "    \"\"\"\n",
    "    \n",
    "    face_detector = dlib.get_frontal_face_detector() # Initialize the face detector using dlib\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)  # Convert the frame to grayscale\n",
    "    faces = face_detector(gray, 1) # Detect faces in the grayscale image\n",
    "\n",
    "    if faces: return True  # Faces detected\n",
    "\n",
    "    return False  # No faces detected\n",
    "\n",
    "def detect_faces(username: str):\n",
    "\n",
    "    \"\"\"\n",
    "    Process a video to detect and save faces in frames.\n",
    "\n",
    "    Parameters:\n",
    "    - username: User identifier used for output folder\n",
    "\n",
    "    Returns:\n",
    "    - True if faces are detected in enough frames, False otherwise\n",
    "    \"\"\"\n",
    "\n",
    "    output_directory = os.path.join('..', 'UserFaces', username) # Set the output directory for storing face images\n",
    "    cap = cv2.VideoCapture(os.path.join(output_directory, 'video.avi')) # Open the video file for processing\n",
    "\n",
    "    # Check if the video file is opened successfully\n",
    "    if not cap.isOpened():\n",
    "        print(\"Error: Could not open video.\")\n",
    "        return\n",
    "\n",
    "    last_valid_frame = None  # Index of the last valid frame\n",
    "    valid_frames = 0 # Number of valid frames\n",
    "    frame_count = 0\n",
    "    total_num_of_frames = int(cap.get(cv2.CAP_PROP_FRAME_COUNT))\n",
    "\n",
    "    # Loop through each frame in the video\n",
    "    while True:\n",
    "\n",
    "        ret, frame = cap.read() # Read the next frame from the video\n",
    "\n",
    "        # Check if the end of the video is reached\n",
    "        if not ret:\n",
    "            print(\"End of video.\")\n",
    "            break\n",
    "\n",
    "        # Calculate the percentage completion\n",
    "        percentage_completion = (frame_count / total_num_of_frames) * 100\n",
    "        if frame_count % 50 == 0:\n",
    "            print(f\"Progress: {int(percentage_completion)}%\")\n",
    "            game_communicator.send_to_game(f'{int(percentage_completion)}')\n",
    "\n",
    "        # Skip frames if they are too close to the previous valid frame\n",
    "        if (last_valid_frame and frame_count // 15 == last_valid_frame // 15):\n",
    "            frame_count += 1\n",
    "            continue\n",
    "\n",
    "        # Process the frame to detect faces\n",
    "        if process_frame(frame):\n",
    "            # Save the face in the output folder\n",
    "            last_valid_frame = frame_count\n",
    "            valid_frames += 1\n",
    "            cv2.imwrite(os.path.join(output_directory, f'face_{valid_frames}.png'), frame)\n",
    "\n",
    "        frame_count += 1  # Increment the frame count\n",
    "\n",
    "    cap.release() # Release the video capture object\n",
    "\n",
    "    print(f'{valid_frames} Valid frames found!')\n",
    "\n",
    "    # Check if an insufficient number of valid frames are detected\n",
    "    if valid_frames < 10:\n",
    "        return False  # Not enough valid frames\n",
    "\n",
    "    return True  # Faces detected in enough frames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Face Registration\n",
    "\n",
    "Through this function we can put all the pieces toghether, hence acquire the video and save the detected faces into the user's directory."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def face_registration(username: str):\n",
    "    \n",
    "    \"\"\"\n",
    "    Main function for face registration.\n",
    "\n",
    "    Parameters:\n",
    "    - username: User identifier for creating output folder\n",
    "\n",
    "    Returns:\n",
    "    - Result of face detection (True if faces are detected, False otherwise)\n",
    "    \"\"\"\n",
    "\n",
    "    video_acquisition(username)\n",
    "\n",
    "    # Perform face detection on the captured video\n",
    "    result = detect_faces(username)\n",
    "\n",
    "    # Remove the temporary video file\n",
    "    os.remove(os.path.join('..', 'UserFaces', username, 'video.avi'))\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Face Recognition (User Login)**\n",
    "\n",
    "For the face recognition part, we will import from the Modules the face_recognition function. Note that the `face_recognition.py` is the Notebook `face_recognition.ipynb` converted into a Python file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.face_recognition import face_recognition \n",
    "from Modules.face_recognition import learn_faces "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Game Controls**\n",
    "\n",
    "The player's character can be controlled in two ways, through the head or using the eyes. The user can freely choose which of the two controls he want to use."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Head Pose Tracker**\n",
    "\n",
    "For head pose tracking, we began with the paper [\"*6D Rotation Representation For Unconstrained Head Pose Estimation*\"](https://paperswithcode.com/paper/6d-rotation-representation-for-unconstrained) available on [Papers With Code](https://paperswithcode.com/). The implementation of this paper, found on [GitHub](https://github.com/thohemp/6DRepNet/tree/master), utilizes a RepVGG architecture (similar to ResNet) to predict the pitch, roll, and yaw of faces in a given frame.\n",
    "\n",
    "#### Overview\n",
    "\n",
    "Our contribution involves creating a `HeadPoseTracker` class designed for use during a game to poll the model and send commands based on the detected head pose. The original repository provided a `demo.py` script capable of face detection and rendering a cube based on the face's orientation. To align the system with our needs, we leveraged the predicted pitch and yaw angles (as roll was unnecessary for our purposes) to determine if the user was looking right, left, up, or down.\n",
    "\n",
    "#### Improved Alignment\n",
    "\n",
    "The initial system assumed that the camera was perfectly aligned with the user's face. To enhance the system's performance, we introduced a preliminary phase where the user looks directly at the screen. During this phase, we detect the pitch and yaw for 100 consecutive frames. The means of the yaw and pitch during this calibration phase are considered as the 'origins' for subsequent tracking. This enhancement allows the system to maintain high precision in detecting the direction of the user's gaze, even if the camera is not perfectly aligned with the face.\n",
    "\n",
    "#### Usage\n",
    "\n",
    "The `HeadPoseTracker` class is designed to be utilized in a gaming environment. It continuously tracks head pose and updates the game state based on the user's head direction. The tracking accuracy is improved by the calibration phase, ensuring reliable performance even in scenarios where the camera alignment with the face is not ideal.\n",
    "\n",
    "For implementation details, refer to the provided Python code and associated modules.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.head_pose_tracker import HeadPoseTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Gaze Tracking**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.gaze_tracker import GazeTracker"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Game Communication**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from Modules.game_communicator import GameCommunicator\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"../Models/shape_predictor_68_face_landmarks.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(\"../Models/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "known_faces = learn_faces(face_detector, shape_predictor, face_encoder)\n",
    "\n",
    "game_communicator = GameCommunicator()\n",
    "game_communicator.wait_for_connection()\n",
    "\n",
    "controls = None\n",
    "\n",
    "while True:\n",
    "\n",
    "    msg = game_communicator.receive_from_game()\n",
    "    msg = msg.split(':')\n",
    "\n",
    "    if msg[0] == \"face_registration\":\n",
    "        username = msg[1]\n",
    "        if(face_registration(username)):\n",
    "            game_communicator.send_to_game(\"success\", True)\n",
    "        else:\n",
    "            game_communicator.send_to_game(\"fail\", True)\n",
    "\n",
    "    if msg[0] == \"face_recognition\":\n",
    "\n",
    "        username = face_recognition(known_faces, face_detector, shape_predictor, face_encoder)\n",
    "        game_communicator.send_to_game(username)\n",
    "    \n",
    "    if msg[0] == \"game_controls\" and msg[1] == \"head_tracking\":\n",
    "        controls = HeadPoseTracker(game_communicator)\n",
    "        game_communicator.send_to_game(\"success\", True)\n",
    "    \n",
    "    if msg[0] == \"game_controls\" and msg[1] == \"eye_tracking\":\n",
    "        controls = GazeTracker(game_communicator)\n",
    "        game_communicator.send_to_game(\"success\", True)\n",
    "\n",
    "    if msg[0] == \"calibration\":\n",
    "\n",
    "        controls.calibrate()\n",
    "        game_communicator.send_to_game(\"success\", True)\n",
    "        \n",
    "    if msg[0] == \"start_playing\":\n",
    "\n",
    "        threading.Thread(target=controls.play).start()\n",
    "        "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
