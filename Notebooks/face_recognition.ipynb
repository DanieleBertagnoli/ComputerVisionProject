{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installing Requirements**\n",
    "\n",
    "Since the project is developed by different people, we will install all the requirements using the requirements.txt file which specifies all the packets' version that must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: dlib==19.24.2 in /home/daniele/Documents/University/ComputerScience/SY/FS/ComputerVision/ComputerVisionProject/project-venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 1)) (19.24.2)\n",
      "Requirement already satisfied: numpy==1.26.2 in /home/daniele/Documents/University/ComputerScience/SY/FS/ComputerVision/ComputerVisionProject/project-venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 2)) (1.26.2)\n",
      "Requirement already satisfied: opencv-python==4.8.1.78 in /home/daniele/Documents/University/ComputerScience/SY/FS/ComputerVision/ComputerVisionProject/project-venv/lib/python3.10/site-packages (from -r ../requirements.txt (line 3)) (4.8.1.78)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Downloading Files from GDrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Face Recognition**\n",
    "\n",
    "Face recognition is a computer vision task that involves identifying and verifying a person's identity based on their facial features. This process can be broken down into these steps:\n",
    "\n",
    "1. **Detection**: Identifying faces in images or video frames.\n",
    "2. **Feature** Extraction: Capturing unique facial characteristics.\n",
    "3. **Representation**: Creating a distinctive template for each face.\n",
    "4. **Model Training**: Associating templates with known identities during training.\n",
    "5. **Matching**: Comparing a new face's template to stored ones for identification.\n",
    "6. **Decision**: Determining a match based on a similarity threshold.\n",
    "\n",
    "Nowadays, these steps are performed through deep learning models. In the following section we will provide a simple implementation through a pre-trained model and our paper implementation (further details in the next sections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Load all the user faces**\n",
    "\n",
    "All the registered users' face are saved into the UserFaces folder. We have to build our \"dataset\" of faces on which the recognition will be performed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Daniele'])\n",
      "{'Daniele': [array([-0.02986787, -0.04006093,  0.00694538, -0.04759768, -0.10049356,\n",
      "        0.02562953, -0.01679501, -0.08864886,  0.18726619, -0.03456683,\n",
      "        0.18501316, -0.03232293, -0.19114077,  0.0444179 , -0.02776724,\n",
      "        0.05532715, -0.05235158, -0.101264  , -0.11612409, -0.11691881,\n",
      "       -0.10209946, -0.03569508, -0.04508139,  0.09246112, -0.19649988,\n",
      "       -0.17695811, -0.12603174, -0.17035078,  0.02675712, -0.12949145,\n",
      "       -0.11669026,  0.05573684, -0.13280265, -0.10324054,  0.07697368,\n",
      "        0.0656424 , -0.07010015, -0.08588725,  0.21078998,  0.03849274,\n",
      "       -0.105121  ,  0.04550758,  0.09148104,  0.27920941,  0.16162728,\n",
      "        0.04018471,  0.00549568, -0.05843762,  0.17402168, -0.22020508,\n",
      "        0.13924021,  0.14540714,  0.21312988,  0.08172394,  0.12143619,\n",
      "       -0.16095696,  0.02420933,  0.11900446, -0.18575825,  0.15674809,\n",
      "        0.08407095,  0.01523454, -0.03249693, -0.08601876,  0.17967992,\n",
      "        0.15243264, -0.09245876, -0.09990194,  0.16179943, -0.1323725 ,\n",
      "       -0.0085481 ,  0.06746304, -0.1462141 , -0.23250143, -0.17133622,\n",
      "        0.10989694,  0.37439016,  0.19454306, -0.15699168,  0.02374151,\n",
      "       -0.06638242, -0.0957111 , -0.01581564, -0.01055845, -0.09040924,\n",
      "       -0.02540265, -0.00482212,  0.05261765,  0.23321596, -0.02418995,\n",
      "        0.06684134,  0.16579017, -0.03337706, -0.0706879 ,  0.0043764 ,\n",
      "        0.03291663, -0.14707859,  0.01826676, -0.09468248,  0.01057334,\n",
      "        0.05778378, -0.11254317,  0.08357446,  0.06040569, -0.25137851,\n",
      "        0.12310189, -0.0184369 , -0.09740625, -0.02123402,  0.00391459,\n",
      "       -0.12476418, -0.00545825,  0.24033567, -0.29727331,  0.17909724,\n",
      "        0.10593595,  0.06509417,  0.07024964,  0.00556142,  0.04329674,\n",
      "        0.04699989,  0.01279149, -0.11990996, -0.07960369,  0.03337798,\n",
      "       -0.06474741,  0.0031897 ,  0.05599698])]}\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "def face_rects(image, face_detector):\n",
    "    # convert the image to grayscale\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    # detect faces in the grayscale image\n",
    "    rects = face_detector(gray, 1)\n",
    "    # return the bounding boxes\n",
    "    return rects\n",
    "\n",
    "\n",
    "def face_landmarks(image, shape_predictor, face_detector):\n",
    "    return [shape_predictor(image, face_rect) for face_rect in face_rects(image, face_detector)]\n",
    "\n",
    "\n",
    "def face_encodings(image, face_encoder, shape_predictor, face_detector):\n",
    "    # compute the facial embeddings for each face \n",
    "    # in the input image. the `compute_face_descriptor` \n",
    "    # function returns a 128-d vector that describes the face in an image\n",
    "    return [np.array(face_encoder.compute_face_descriptor(image, face_landmark)) \n",
    "            for face_landmark in face_landmarks(image, shape_predictor, face_detector)]\n",
    "\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"../Models/shape_predictor_68_face_landmarks.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(\"../Models/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "# Initialize lists to store known faces and their corresponding names\n",
    "known_faces = {}\n",
    "\n",
    "base_directory = \"../UserFaces/\" # Directory containing user faces\n",
    "\n",
    "# Iterate through directories\n",
    "for user_name in os.listdir(base_directory):\n",
    "\n",
    "    user_path = os.path.join(base_directory, user_name)\n",
    "\n",
    "    # Iterate through face images in each user directory\n",
    "    for filename in os.listdir(user_path):\n",
    "        image_path = os.path.join(user_path, filename)\n",
    "\n",
    "        img = cv2.imread(image_path) # Read the image\n",
    "        new_encodings = face_encodings(img, face_encoder, shape_predictor, face_detector)\n",
    "\n",
    "        encodings = known_faces.get(user_name, [])\n",
    "        encodings.extend(new_encodings)\n",
    "        known_faces[user_name] = encodings\n",
    "\n",
    "\n",
    "print(known_faces.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-trained Model**\n",
    "\n",
    "This model is provided by the dlib library. Dlib's face recognition model is based on classical machine learning techniques rather than deep learning. It uses a combination of HOG (Histogram of Oriented Gradients) features and a SVM (Support Vector Machine) classifier to identify and recognize faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Obtain Users' Face Embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cap = cv2.VideoCapture(0) # Open a connection to the webcam (0 represents the default camera)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() # Read a frame from the webcam\n",
    "\n",
    "    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY) # Convert the frame to grayscale for face detection\n",
    "    \n",
    "    faces = detector(gray) # Detect faces in the frame\n",
    "\n",
    "    # Iterate over detected faces\n",
    "    for face in faces:\n",
    "        landmarks = face_recognizer(frame, face) # Get facial landmarks\n",
    "\n",
    "        # Compare with known faces\n",
    "        for i, known_face in enumerate(known_faces):\n",
    "\n",
    "            if landmarks == known_face:\n",
    "                name = known_names[i]\n",
    "                print(f\"Hello, {name}!\")\n",
    "\n",
    "        # Draw a rectangle around the face\n",
    "        x, y, w, h = face.left(), face.top(), face.width(), face.height()\n",
    "        cv2.rectangle(frame, (x, y), (x + w, y + h), (0, 255, 0), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # Break the loop if the 'q' key is pressed\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
