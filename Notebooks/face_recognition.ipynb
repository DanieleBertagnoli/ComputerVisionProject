{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Installing Requirements**\n",
    "\n",
    "Since the project is developed by different people, we will install all the requirements using the requirements.txt file which specifies all the packets' version that must be installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r ../requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Downloading Files from GDrive**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Face Recognition**\n",
    "\n",
    "Face recognition is a computer vision task that involves identifying and verifying a person's identity based on their facial features. This process can be broken down into these steps:\n",
    "\n",
    "1. **Detection**: Identifying faces in images or video frames.\n",
    "2. **Feature** Extraction: Capturing unique facial characteristics.\n",
    "3. **Representation**: Creating a distinctive template for each face.\n",
    "4. **Model Training**: Associating templates with known identities during training.\n",
    "5. **Matching**: Comparing a new face's template to stored ones for identification.\n",
    "6. **Decision**: Determining a match based on a similarity threshold.\n",
    "\n",
    "Nowadays, these steps are performed through deep learning models. In the following section we will provide a simple implementation through a pre-trained model and our paper implementation (further details in the next sections)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Pre-trained Model**\n",
    "\n",
    "This model is provided by the dlib library. Dlib's face recognition model is based on classical machine learning techniques rather than deep learning. It uses a combination of HOG (Histogram of Oriented Gradients) features and a SVM (Support Vector Machine) classifier to identify and recognize faces."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Load all the user faces**\n",
    "\n",
    "Now we will define functions to detect faces, extract facial landmarks, and compute facial encodings. We will save the usefull information into a dict that represents the known users."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import cv2\n",
    "import dlib\n",
    "import numpy as np\n",
    "\n",
    "def face_rects(image, face_detector):\n",
    "    gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY) # Convert the image to grayscale\n",
    "    rects = face_detector(gray, 1) # Detect faces in the grayscale image\n",
    "\n",
    "    return rects\n",
    "\n",
    "\n",
    "def face_landmarks(image, shape_predictor, face_detector):\n",
    "    # Compute the face landmarks\n",
    "    return [shape_predictor(image, face_rect) for face_rect in face_rects(image, face_detector)]\n",
    "\n",
    "\n",
    "def face_encodings(image, face_encoder, shape_predictor, face_detector):\n",
    "    # Compute the facial embeddings for each face (128-d vector that describes the face in an image)\n",
    "    return [np.array(face_encoder.compute_face_descriptor(image, face_landmark)) for face_landmark in face_landmarks(image, shape_predictor, face_detector)]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "face_detector = dlib.get_frontal_face_detector()\n",
    "shape_predictor = dlib.shape_predictor(\"../Models/shape_predictor_68_face_landmarks.dat\")\n",
    "face_encoder = dlib.face_recognition_model_v1(\"../Models/dlib_face_recognition_resnet_model_v1.dat\")\n",
    "\n",
    "known_faces = {} # Dict that will store the user's embeddings\n",
    "\n",
    "base_directory = \"../UserFaces/\" # Directory containing user faces\n",
    "\n",
    "# Iterate through directories\n",
    "for user_name in os.listdir(base_directory):\n",
    "\n",
    "    user_path = os.path.join(base_directory, user_name)\n",
    "\n",
    "    # Iterate through face images in each user directory\n",
    "    for filename in os.listdir(user_path):\n",
    "        image_path = os.path.join(user_path, filename)\n",
    "\n",
    "        img = cv2.imread(image_path) # Read the image\n",
    "        new_encodings = face_encodings(img, face_encoder, shape_predictor, face_detector) # Get the embeddings\n",
    "\n",
    "        encodings = known_faces.get(user_name, []) \n",
    "        encodings.extend(new_encodings) # Add the embeddings to the already saved ones\n",
    "        known_faces[user_name] = encodings\n",
    "\n",
    "\n",
    "print(known_faces.keys())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **User Recognition**\n",
    "\n",
    "In this part we will use the obtained information perform a real-time recognition through the webcam."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def nb_of_matches(known_encodings, unknown_encoding):\n",
    "    # compute the Euclidean distance between the current face encoding \n",
    "    # and all the face encodings in the database\n",
    "    distances = np.linalg.norm(known_encodings - unknown_encoding, axis=1)\n",
    "    # keep only the distances that are less than the threshold\n",
    "    small_distances = distances <= 0.6\n",
    "    # return the number of matches\n",
    "    return sum(small_distances)\n",
    "\n",
    "\n",
    "cap = cv2.VideoCapture(0) # Open a connection to the webcam (0 represents the default camera)\n",
    "\n",
    "while True:\n",
    "    ret, frame = cap.read() # Read a frame from the webcam\n",
    "\n",
    "    frame_encodings = face_encodings(frame, face_detector=face_detector, face_encoder=face_encoder, shape_predictor=shape_predictor)\n",
    "    names = []\n",
    "\n",
    "    for encoding in frame_encodings:\n",
    "        counts = {}\n",
    "\n",
    "        for (name, known_encodings) in known_faces.items():\n",
    "            counts[name] = nb_of_matches(known_encodings, encoding)\n",
    "        \n",
    "        if all(count == 0 for count in counts.values()):\n",
    "            name = \"Unknown\"\n",
    "        else:\n",
    "            name = max(counts, key=counts.get)\n",
    "\n",
    "        names.append(name)\n",
    "\n",
    "    for rect, name in zip(face_rects(frame, face_detector), names):\n",
    "        # get the bounding box for each face using the `rect` variable\n",
    "        x1, y1, x2, y2 = rect.left(), rect.top(), rect.right(), rect.bottom()\n",
    "        # draw the bounding box of the face along with the name of the person\n",
    "        cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "        cv2.putText(frame, name, (x1, y1 - 10), \n",
    "                cv2.FONT_HERSHEY_SIMPLEX, 0.75, (0, 255, 0), 2)\n",
    "\n",
    "    # Display the result\n",
    "    cv2.imshow(\"Face Recognition\", frame)\n",
    "\n",
    "    if cv2.waitKey(1) & 0xFF == ord('q'): # Break the loop if the 'q' key is pressed\n",
    "        break\n",
    "\n",
    "# Release the webcam and close all windows\n",
    "cap.release()\n",
    "cv2.destroyAllWindows()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project-venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
